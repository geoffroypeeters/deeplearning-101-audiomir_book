{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTO Cover-Detection based on Triplet-Loss\n",
    "\n",
    "- date: 2024-07-16\n",
    "- author: geoffroy.peeters@telecom-paris.fr\n",
    "\n",
    "code based on \n",
    "- MOVE https://arxiv.org/pdf/1910.12551 https://github.com/furkanyesiler/move\n",
    "\n",
    "\n",
    "using datasets\n",
    "- Cover-1000 https://www.covers1000.net/dataset.html\n",
    "- DA-TACOS https://github.com/MTG/da-tacos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/peeters/opt/anaconda3/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so, 6): Symbol not found: __ZNK3c104Type14isSubtypeOfExtERKNSt3__110shared_ptrIS0_EEPNS1_13basic_ostreamIcNS1_11char_traitsIcEEEE\n  Referenced from: /Users/peeters/opt/anaconda3/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in: /Users/peeters/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n in /Users/peeters/opt/anaconda3/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchaudio/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     compliance,\n\u001b[1;32m      4\u001b[0m     datasets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     transforms,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     list_audio_backends,\n\u001b[1;32m     16\u001b[0m     get_audio_backend,\n\u001b[1;32m     17\u001b[0m     set_audio_backend,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchaudio/_extension.py:27\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchaudio  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchaudio/_extension.py:21\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# In case `torchaudio` is deployed with `pex` format, this file does not exist.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# In this case, we expect that `libtorchaudio` is available somewhere\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# in the search path of dynamic loading mechanism, and importing `_torchaudio`,\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# which depends on `libtorchaudio` and dynamic loader will handle it for us.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_ops.py:933\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    928\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/peeters/opt/anaconda3/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so, 6): Symbol not found: __ZNK3c104Type14isSubtypeOfExtERKNSt3__110shared_ptrIS0_EEPNS1_13basic_ostreamIcNS1_11char_traitsIcEEEE\n  Referenced from: /Users/peeters/opt/anaconda3/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in: /Users/peeters/opt/anaconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n in /Users/peeters/opt/anaconda3/lib/python3.8/site-packages/torchaudio/lib/libtorchaudio.so"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import pprint as pp\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['100259', '100444', '100445', '100977', '100978', '10118', '10119', '101628', '101988', '10255', '10339', '103484', '103816', '105462', '105475', '105666', '105979', '107456', '108638', '108887', '108889', '109014', '109016', '1091', '10985', '110690', '110944', '11129', '11130', '111805', '111994', '112723', '112724', '112885', '114871', '11579', '115846', '115847', '11591', '116937', '116947', '116948', '116950', '117290', '117291', '117560', '117561', '117837', '118446', '120242', '120243', '120700', '12174', '123058', '12396', '124152', '126122', '126344', '126867', '126868', '128413', '128891', '128894', '1298', '129901', '129902', '13048', '13110', '131132', '13134', '131709', '132716', '133360', '134373', '135909', '136091', '136463', '137850', '137851', '137985', '143720', '14444', '14474', '144746', '145985', '147843', '147845', '147896', '148228', '148555', '149756', '149761', '14977', '14978', '15045', '15046', '150936', '150972', '152045', '15248', '15249', '152998', '152999', '153503', '153504', '154270', '15456', '15457', '15605', '156953', '157446', '15790', '159332', '161285', '162110', '162300', '162470', '163689', '163690', '163878', '1641', '16670', '16671', '167212', '167402', '167403', '16762', '167910', '168060', '16957', '16958', '17105', '17106', '173263', '173770', '174702', '174724', '174725', '177632', '177888', '177889', '178287', '178398', '179542', '17971', '17981', '18048', '180795', '180796', '18240', '183462', '184074', '184199', '184200', '190113', '190114', '191233', '191983', '193541', '193558', '19515', '195274', '196362', '196374', '19756', '19758', '198685', '201340', '201998', '202256', '20304', '204059', '204061', '204396', '204408', '204411', '207373', '207374', '207866', '20841', '20842', '20853', '20854', '209852', '209855', '210054', '210543', '210544', '211139', '211433', '211634', '211635', '21202', '212237', '213110', '21330', '214148', '214243', '21709', '21710', '217256', '217495', '217496', '218795', '218796', '219325', '219326', '220512', '221090', '222191', '222193', '222809', '223496', '223497', '224892', '22508', '22509', '225146', '225225', '225226', '226147', '22624', '22625', '226716', '226988', '226990', '227234', '227252', '227275', '228823', '230522', '230523', '230594', '231485', '231524', '231605', '231607', '233810', '23388', '234025', '23403', '23404', '234746', '234747', '23802', '238454', '238978', '239798', '239799', '240036', '240037', '240320', '240357', '240683', '241457', '241458', '241493', '243859', '244695', '244972', '244975', '245097', '245179', '246131', '246133', '246563', '246996', '246997', '24705', '24706', '247396', '247399', '247618', '247619', '247638', '248090', '248093', '248698', '248701', '252297', '252298', '252881', '253081', '253082', '253217', '253404', '253411', '253412', '25449', '25451', '254534', '254549', '254905', '254914', '254929', '255329', '255339', '25541', '25543', '256306', '256307', '256308', '256792', '256793', '25706', '25707', '257334', '258521', '25886', '25893', '259337', '259970', '261001', '261004', '261091', '261092', '261803', '261902', '262480', '263197', '263203', '263206', '263287', '263288', '263289', '263313', '263314', '263552', '263554', '263799', '263802', '26539', '265424', '265434', '265435', '266457', '267062', '269215', '269216', '269405', '269958', '270196', '270198', '270231', '270232', '270233', '272837', '273069', '273070', '273644', '273755', '273756', '275168', '275476', '276805', '276941', '277413', '27881', '27898', '279298', '279422', '27948', '279484', '279485', '27949', '2800', '28014', '280253', '280258', '28080', '28081', '281225', '282310', '282571', '28310', '283142', '28333', '284092', '284120', '28507', '28510', '285904', '285906', '285907', '285911', '28642', '286508', '286509', '287648', '288782', '288788', '289004', '289250', '289258', '290387', '290663', '291617', '291655', '291656', '291889', '29321', '293738', '293989', '294713', '294714', '295112', '295114', '295221', '296614', '296615', '296617', '297265', '297558', '297559', '298502', '298506', '298696', '29975', '29976', '300135', '300137', '301656', '301658', '301664', '302060', '302099', '302100', '302570', '303341', '305075', '30554', '30555', '30600', '30601', '306343', '306344', '307554', '307632', '307633', '307665', '308145', '308146', '309236', '309237', '30952', '309782', '310050', '310052', '310474', '310475', '310778', '312118', '312467', '313', '313541', '313583', '314', '31466', '31468', '316388', '316414', '316509', '316511', '316517', '316522', '316809', '316810', '316813', '316814', '316951', '316962', '317047', '317048', '317323', '317333', '317360', '317449', '317450', '317453', '317708', '318043', '318657', '318658', '319101', '319104', '320507', '320895', '320896', '321305', '321306', '32133', '322088', '322089', '322102', '322602', '322605', '322952', '323256', '323644', '323656', '323766', '326806', '326816', '326818', '326819', '326820', '32727', '327394', '327399', '327400', '327587', '328275', '328276', '328300', '330274', '330387', '330388', '330664', '330665', '330798', '33171', '331942', '332686', '332687', '33304', '333838', '334180', '334181', '335554', '335782', '335822', '335824', '335920', '336806', '336807', '337012', '337137', '337139', '337296', '337391', '339729', '340452', '341360', '342077', '342084', '344162', '344203', '344204', '346327', '347048', '347062', '347356', '347991', '347992', '348913', '349472', '349473', '351264', '352229', '352230', '352643', '35434', '355267', '355423', '356121', '357981', '358241', '358592', '359802', '359810', '360355', '360356', '361173', '361174', '361477', '361929', '363711', '364280', '365220', '365486', '365488', '365496', '365891', '365892', '366167', '366248', '366710', '366711', '366884', '367128', '367134', '367261', '368163', '368552', '368553', '368820', '369016', '369259', '369352', '369353', '369790', '369792', '370198', '370200', '370206', '370313', '370314', '371160', '371434', '371735', '372033', '373178', '373738', '374183', '374253', '375789', '376166', '377125', '377133', '377256', '377258', '37861', '378930', '380490', '38201', '383216', '383415', '38361', '38362', '383706', '383707', '38451', '385214', '385289', '385563', '385564', '387442', '387446', '387447', '387610', '388093', '388155', '388577', '389247', '38954', '38955', '390655', '39089', '39090', '392127', '392128', '393025', '393228', '393229', '393246', '393732', '393818', '39404', '394765', '395043', '395307', '395340', '395342', '395458', '395832', '396', '396200', '396202', '396209', '397024', '398120', '398151', '398848', '399091', '399247', '399249', '399466', '399629', '399630', '400785', '400786', '401346', '401348', '403057', '403633', '404946', '405609', '406975', '40775', '40826', '408914', '40987', '409906', '409907', '411176', '413535', '416257', '416505', '418136', '419663', '42092', '42234', '422437', '422438', '423040', '423044', '423045', '423046', '423063', '423991', '423993', '423994', '425822', '425828', '426460', '426762', '42696', '42772', '42773', '428003', '430392', '430503', '430510', '43138', '433977', '434423', '434778', '434933', '43502', '435445', '435446', '435783', '436546', '436812', '436982', '436983', '437070', '437071', '43753', '43754', '438190', '438817', '438887', '439020', '439195', '439202', '439216', '439232', '439525', '439816', '440866', '440873', '441477', '44154', '44155', '441953', '442805', '443056', '443058', '44308', '443720', '443723', '443725', '445090', '445811', '446085', '446086', '446087', '446717', '446758', '447191', '447209', '447236', '447271', '450267', '450647', '450648', '450664', '450668', '451379', '451390', '451394', '452494', '45426', '45427', '454571', '454789', '454805', '45550', '455714', '457274', '457298', '458428', '458435', '458810', '460437', '460820', '461791', '461792', '464578', '464579', '464599', '464600', '464894', '465100', '465102', '466399', '466402', '466549', '466550', '466631', '466790', '466791', '467468', '467475', '468031', '468672', '468674', '468680', '468689', '469027', '471533', '471685', '471686', '471687', '47184', '472150', '472259', '472263', '472527', '472878', '473621', '474292', '475069', '475371', '476431', '476811', '477185', '477854', '480572', '480573', '480575', '481064', '481065', '481258', '481626', '481688', '481913', '482566', '482567', '483994', '484952', '485375', '485390', '486002', '486061', '486189', '486190', '486253', '486254', '486269', '486644', '487500', '487576', '488308', '488341', '488557', '488587', '49065', '49574', '49575', '49775', '50224', '50352', '51320', '51464', '51465', '51527', '51528', '51782', '53165', '53169', '53471', '53483', '53532', '53533', '53574', '53603', '54029', '55189', '55327', '56202', '56206', '56421', '57395', '58603', '58604', '58941', '58942', '59021', '59219', '59569', '60146', '60765', '60766', '61636', '61955', '62165', '62178', '62318', '636', '637', '63718', '63719', '64045', '66311', '67258', '67378', '67416', '67568', '68097', '68249', '68340', '68917', '69116', '69132', '70002', '70011', '70592', '70699', '70714', '71463', '71465', '72203', '72204', '72340', '72445', '72446', '73590', '73591', '74026', '74587', '74971', '75009', '76366', '76367', '76637', '77165', '77443', '77444', '77620', '78085', '78449', '78614', '78939', '79256', '79257', '80930', '82999', '83001', '83196', '83197', '83837', '84282', '87382', '87832', '88014', '88725', '88726', '88727', '90340', '91416', '91418', '91650', '91827', '91828', '91946', '92488', '92489', '93329', '93616', '94020', '94051', '95304', '95663', '95932', '96120', '96146', '97333', '97334', '98704', '98710', '98717', '99212', '99763', '99766']>\n"
     ]
    }
   ],
   "source": [
    "ROOT = '/tsi/data_doctorants/gpeeters/_data/'\n",
    "base = 'cover1000'\n",
    "\n",
    "pyjama_annot_file = f'{ROOT}/{base}.pyjama'\n",
    "hdf5_feat_file = f'{ROOT}/{base}_feat.hdf5'\n",
    "\n",
    "with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid:\n",
    "    data_d = json.load(json_fid)\n",
    "audiofile_l = [entry['filepath'][0]['value'] for entry in data_d['collection']['entry']]\n",
    "\n",
    "with h5py.File(hdf5_feat_file, 'r') as hdf5_fid:\n",
    "    print(hdf5_fid['/'].keys())\n",
    "    #for idx, audio_file in enumerate(audiofile_l):\n",
    "    #    print(f\"{idx} shape: {hdf5_fid[audio_file].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "param_d = Namespace()\n",
    "\n",
    "param_d.num_of_labels = 16\n",
    "\n",
    "param_d.emb_size = 32   #16000\n",
    "param_d.sum_method = 4\n",
    "param_d.final_activation = 3\n",
    "param_d.downsampling_parameters = 2\n",
    "\n",
    "param_d.margin =  1.0\n",
    "param_d.mining_strategy = 1\n",
    "param_d.norm_dist = 1\n",
    "param_d.lr = 0.1\n",
    "param_d.momentum = 0\n",
    "param_d.nb_epoch = 100\n",
    "\n",
    "param_d.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(param_d.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid: data_d = json.load(json_fid)\n",
    "entry_l = data_d['collection']['entry']\n",
    "performanceid_l  = set([entry['performance-id'][0]['value'] for entry in entry_l])\n",
    "len(performanceid_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverDataset(Dataset):\n",
    "    \"\"\"\n",
    "    description\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hdf5_feat_file, pyjama_annot_file, do_train):\n",
    "        \n",
    "        self.h = 23\n",
    "        self.w = 1800\n",
    "        \n",
    "        with open(pyjama_annot_file, encoding = \"utf-8\") as json_fid: data_d = json.load(json_fid)\n",
    "        entry_l = data_d['collection']['entry']\n",
    "        all_workid_l  = list(set([entry['work-id'][0]['value'] for entry in entry_l]))\n",
    "        performanceid_l  = set([entry['performance-id'][0]['value'] for entry in entry_l])\n",
    "\n",
    "        self.do_train = do_train\n",
    "        if self.do_train:   workid_l = [all_workid_l[idx] for idx in range(len(all_workid_l)) if (idx % 5) != 0]\n",
    "        else:               workid_l = [all_workid_l[idx] for idx in range(len(all_workid_l)) if (idx % 5) == 0]\n",
    "\n",
    "        self.workid_to_perfomanceid_d = {}\n",
    "        for workid in workid_l:\n",
    "            self.workid_to_perfomanceid_d[workid] = [entry['performance-id'][0]['value'] for entry in entry_l if entry['work-id'][0]['value']==workid]\n",
    "\n",
    "        self.clique_list_l = []\n",
    "        for workid in self.workid_to_perfomanceid_d.keys():\n",
    "            # --- check the number of performanceid for each workid\n",
    "            # --- if this number is large this workid will be present in many clique\n",
    "            nb_performanceid = len(self.workid_to_perfomanceid_d[workid])\n",
    "            if nb_performanceid < 2:     pass\n",
    "            elif nb_performanceid < 6:   self.clique_list_l.extend([workid] * 1)\n",
    "            elif nb_performanceid < 10:  self.clique_list_l.extend([workid] * 2)\n",
    "            elif nb_performanceid < 14:  self.clique_list_l.extend([workid] * 3)\n",
    "            else:                       self.clique_list_l.extend([workid] * 4)\n",
    "\n",
    "        self.data_d = {}\n",
    "        with h5py.File(hdf5_feat_file, 'r') as feat_fid:\n",
    "            for workid in self.workid_to_perfomanceid_d.keys():\n",
    "                # --- get the list of performanceid assoicated to this workid\n",
    "                perfomanceid_l = self.workid_to_perfomanceid_d[workid]\n",
    "                for perfomanceid in perfomanceid_l:\n",
    "                    self.data_d[int(perfomanceid)] = {}\n",
    "                    self.data_d[int(perfomanceid)]['workid'] = workid\n",
    "                    self.data_d[int(perfomanceid)]['perfomanceid'] = perfomanceid\n",
    "                    # --- Get data and convert to make rotation invariant -> self.data_d (1, 23, 1800)\n",
    "                    data = torch.from_numpy(feat_fid['/' +  str(perfomanceid) + '/'][:].T).unsqueeze(0)\n",
    "                    self.data_d[int(perfomanceid)]['value'] = torch.concatenate((data, data), dim=1)[:,:-1,:]\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.clique_list_l)\n",
    "    \n",
    "    def getitem_by_performanceid(self, perfomanceid):\n",
    "        item = self.data_d[perfomanceid]['value']\n",
    "        # if the song is longer than the required width, choose a random start point to crop\n",
    "        if item.shape[2] >= self.w: item = item[:, :, 0:self.w]\n",
    "        else:                       item = torch.cat((item, torch.zeros([1, self.h, self.w - item.shape[2]])), 2)\n",
    "        return item, self.data_d[perfomanceid]['workid']\n",
    "\n",
    "    def __getitem__(self, clique_idx):\n",
    "        \n",
    "        label = self.clique_list_l[clique_idx]  # getting the clique chosen by the dataloader\n",
    "\n",
    "        # selecting 4 songs from the given clique\n",
    "        if len(self.workid_to_perfomanceid_d[label]) == 2:  # if the clique size is 2, repeat the already selected songs\n",
    "            idx1, idx2 = np.random.choice(self.workid_to_perfomanceid_d[label], 2, replace=False)\n",
    "            idx3, idx4 = idx1, idx2\n",
    "        elif len(self.workid_to_perfomanceid_d[label]) == 3:  # if the clique size is 3, choose one of the songs twice\n",
    "            idx1, idx2, idx3 = np.random.choice(self.workid_to_perfomanceid_d[label], 3, replace=False)\n",
    "            idx4 = np.random.choice(self.workid_to_perfomanceid_d[label], 1, replace=False)[0]\n",
    "        else:  # if the clique size is larger than or equal to 4, choose 4 songs randomly\n",
    "            idx1, idx2, idx3, idx4 = np.random.choice(self.workid_to_perfomanceid_d[label], 4, replace=False)\n",
    "        \n",
    "        items = []\n",
    "        for idx in [idx1, idx2, idx3, idx4]:\n",
    "            item, _ = self.getitem_by_performanceid(idx)\n",
    "            items.append(item)\n",
    "        \n",
    "        return torch.stack(items, 0), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_mining_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for triplet mining\n",
    "    :param batch: elements of the mini-batch (pcp features and labels)\n",
    "    :return: collated elements\n",
    "    \"\"\"\n",
    "    items = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    return torch.cat(items, 0), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 23, 1800])\n",
      "2\n",
      "torch.Size([64, 1, 23, 1800])\n",
      "[350, 73, 337, 139, 393, 83, 175, 162, 294, 250, 92, 99, 307, 88, 364, 239]\n"
     ]
    }
   ],
   "source": [
    "train_set = CoverDataset(hdf5_feat_file, pyjama_annot_file, do_train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=param_d.num_of_labels, shuffle=True, collate_fn=triplet_mining_collate, drop_last=True)\n",
    "\n",
    "items, label = train_set[0]\n",
    "print(items.size())\n",
    "print(label)\n",
    "\n",
    "items, labels = next(iter(train_loader))\n",
    "print(items.size())\n",
    "print(labels)\n",
    "\n",
    "valid_set = CoverDataset(hdf5_feat_file, pyjama_annot_file, do_train=False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=param_d.num_of_labels, shuffle=False, collate_fn=triplet_mining_collate, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOVEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model object for MOVE.\n",
    "    The explanation of the design choices can be found at https://arxiv.org/abs/1910.12551.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, param_d):\n",
    "        \"\"\"\n",
    "        Initializing the network\n",
    "        :param emb_size: the size of the final embeddings produced by the model\n",
    "        :param sum_method: the summarization method for the model\n",
    "        :param final_activation: final activation to use for the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.prelu1 = nn.PReLU(init=0.01)\n",
    "        self.prelu2 = nn.PReLU(init=0.01)\n",
    "        self.prelu3 = nn.PReLU(init=0.01)\n",
    "        self.prelu4 = nn.PReLU(init=0.01)\n",
    "        self.prelu5 = nn.PReLU(init=0.01)\n",
    "\n",
    "        N = 4\n",
    "        self.n1 = int(256/param_d.downsampling_parameters)\n",
    "        self.n2 = int(256/param_d.downsampling_parameters)\n",
    "        self.n3 = int(256/param_d.downsampling_parameters)\n",
    "        self.n4 = int(256/param_d.downsampling_parameters)\n",
    "        if param_d.sum_method in [0, 1, 2]: \n",
    "            self.n5 = int(256/param_d.downsampling_parameters)\n",
    "            self.n6 = self.n5\n",
    "        else:\n",
    "            self.n5 = int(512/param_d.downsampling_parameters)\n",
    "            self.n6 = int(self.n5/2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.n1, kernel_size=(12, 180), bias=True)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "        self.key_pool = nn.MaxPool2d(kernel_size=(12, 1))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.n1, out_channels=self.n2, kernel_size=(1, 5), bias=True)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=self.n2, out_channels=self.n3, kernel_size=(1, 5), dilation=(1, 20), bias=True)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=self.n3, out_channels=self.n4, kernel_size=(1, 5), bias=True)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=self.n4, out_channels=self.n5, kernel_size=(1, 5), dilation=(1, 13), bias=True)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "        self.fin_emb_size = param_d.emb_size\n",
    "        self.autopool_p = nn.Parameter(torch.tensor(0.).float())\n",
    "        self.sum_method = param_d.sum_method\n",
    "        self.final_activation = param_d.final_activation\n",
    "\n",
    "\n",
    "        lin_bias = True\n",
    "        if self.final_activation == 3:\n",
    "            self.lin_bn = nn.BatchNorm1d(param_d.emb_size, affine=False)\n",
    "            lin_bias = False\n",
    "\n",
    "        self.lin1 = nn.Linear(in_features=self.n6, out_features=param_d.emb_size, bias=lin_bias)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Defining a forward pass of the network\n",
    "        :param data: input tensor for the network\n",
    "        :return: output tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- data (batch, dim=23, time=1800)\n",
    "\n",
    "        x = self.prelu1(self.conv1(data))\n",
    "        x = self.key_pool(x)\n",
    "        \n",
    "        x = self.prelu2(self.conv2(x))\n",
    "        x = self.prelu3(self.conv3(x))\n",
    "        x = self.prelu4(self.conv4(x))\n",
    "        x = self.prelu5(self.conv5(x))\n",
    "        \n",
    "        if self.sum_method == 0:\n",
    "            x = torch.max(x, dim=3, keepdim=True).values\n",
    "        elif self.sum_method == 1:\n",
    "            x = torch.mean(x, dim=3, keepdim=True)\n",
    "        elif self.sum_method == 2:\n",
    "            # --- weights are computed using autopool and the same channels \n",
    "            weights = self.autopool_weights(x)\n",
    "            x = torch.sum(x * weights, dim=3, keepdim=True)\n",
    "        elif self.sum_method == 3:\n",
    "            # --- weights are computed using softmax and the first 256 channels\n",
    "            weights = torch.nn.functional.softmax(x[:, 256:], dim=3)\n",
    "            x = torch.sum(x[:, :256] * weights, dim=3, keepdim=True)\n",
    "        else:\n",
    "            # --- weights are computed using autopool and the first 256 channels\n",
    "            weights = self.autopool_weights(x[:, :int(self.n5/2)])\n",
    "            x = torch.sum(x[:, int(self.n5/2):] * weights, dim=3, keepdim=True)\n",
    "\n",
    "        x = x.view(-1, self.n6)\n",
    "        x = self.lin1(x)\n",
    "\n",
    "        if self.final_activation == 1:      x = torch.sigmoid(x)\n",
    "        elif self.final_activation == 2:    x = torch.tanh(x)\n",
    "        elif self.final_activation == 3:    x = self.lin_bn(x)\n",
    "        else:                               x = x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def autopool_weights(self, data):\n",
    "        \"\"\"\n",
    "        Calculating the autopool weights for a given tensor\n",
    "        :param data: tensor for calculating the softmax weights with autopool\n",
    "        :return: softmax weights with autopool\n",
    "\n",
    "        see https://arxiv.org/pdf/1804.10070\n",
    "        alpha=0: unweighted mean\n",
    "        alpha=1: softmax\n",
    "        alpha=inf: max-pooling\n",
    "        \"\"\"\n",
    "        # --- x: (batch, 256, 1, T)\n",
    "        x = data * self.autopool_p\n",
    "        # --- max_values: (batch, 256, 1, 1)\n",
    "        max_values = torch.max(x, dim=3, keepdim=True).values\n",
    "        # --- softmax (batch, 256, 1, T)\n",
    "        softmax = torch.exp(x - max_values)\n",
    "        # --- weights (batch, 256, 1, T)\n",
    "        weights = softmax / torch.sum(softmax, dim=3, keepdim=True)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 12, 1621]         276,608\n",
      "             PReLU-2        [-1, 128, 12, 1621]               1\n",
      "         MaxPool2d-3         [-1, 128, 1, 1621]               0\n",
      "            Conv2d-4         [-1, 128, 1, 1617]          82,048\n",
      "             PReLU-5         [-1, 128, 1, 1617]               1\n",
      "            Conv2d-6         [-1, 128, 1, 1537]          82,048\n",
      "             PReLU-7         [-1, 128, 1, 1537]               1\n",
      "            Conv2d-8         [-1, 128, 1, 1533]          82,048\n",
      "             PReLU-9         [-1, 128, 1, 1533]               1\n",
      "           Conv2d-10         [-1, 256, 1, 1481]         164,096\n",
      "            PReLU-11         [-1, 256, 1, 1481]               1\n",
      "           Linear-12                   [-1, 32]           4,096\n",
      "      BatchNorm1d-13                   [-1, 32]               0\n",
      "================================================================\n",
      "Total params: 690,949\n",
      "Trainable params: 690,949\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.16\n",
      "Forward/backward pass size (MB): 54.52\n",
      "Params size (MB): 2.64\n",
      "Estimated Total Size (MB): 57.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "move_model = MOVEModel(param_d).to(param_d.device)\n",
    "data, label = next(iter(train_loader))\n",
    "print(move_model(data.to(param_d.device)).size())\n",
    "\n",
    "import torchsummary\n",
    "torchsummary.summary(move_model, (1, 23, 1800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance_matrix(x, y=None, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Calculating squared euclidean distances between the elements of two tensors\n",
    "    :param x: first tensor\n",
    "    :param y: second tensor (optional)\n",
    "    :param eps: epsilon value for avoiding div by zero\n",
    "    :return: pairwise distance matrix\n",
    "    \"\"\"\n",
    "    x_norm = x.pow(2).sum(dim=1).view(-1, 1)\n",
    "    if y is not None:\n",
    "        y_norm = y.pow(2).sum(dim=1).view(1, -1)\n",
    "    else:\n",
    "        y = x\n",
    "        y_norm = x_norm.view(1, -1)\n",
    "\n",
    "    dist = x_norm + y_norm - 2 * torch.mm(x, y.t().contiguous())\n",
    "    return torch.clamp(dist, eps, np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_renumber(labels):\n",
    "    \"\"\"\n",
    "    renumber the labels (which correspond to work-id) starting from 0 and get 4 of them each time\n",
    "    \"\"\"\n",
    "    aux = {}\n",
    "    i_labels = []\n",
    "    for l in labels:\n",
    "        if l not in aux:\n",
    "            aux[l] = len(aux)\n",
    "        i_labels += [aux[l]]*4\n",
    "    return i_labels\n",
    "\n",
    "#f_renumber([300, 200, 500, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss_mining(embedding_m, labels, param_d):\n",
    "    \"\"\"\n",
    "    Online mining function for selecting the triplets\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = embedding_m.size(0)\n",
    "\n",
    "    # creating positive and negative masks for online mining\n",
    "    i_labels = f_renumber(labels)\n",
    "    i_labels = torch.Tensor(i_labels).view(-1, 1).to(param_d.device)\n",
    "    # --- get a ones matrix with zero on main diagonal (to avoid selecting the anchor itself for positive or negative)\n",
    "    mask_diag = (1 - torch.eye(batch_size)).long().to(param_d.device)\n",
    "    # --- the mask with 1 if same work-id 0 otherwise\n",
    "    sameworkid_mask = (pairwise_distance_matrix(i_labels) < 0.5).long()\n",
    "    # --- same work-id but not the anchor\n",
    "    mask_pos = mask_diag * sameworkid_mask\n",
    "    # --- different work-id and not the anchor\n",
    "    mask_neg = mask_diag * (1 - mask_pos)\n",
    "\n",
    "    # getting the pairwise distance matrix\n",
    "    dist_all = pairwise_distance_matrix(embedding_m)  \n",
    "    # normalizing the distances by the embedding size\n",
    "    if param_d.norm_dist == 1:  dist_all /= param_d.emb_size\n",
    "\n",
    "    if param_d.mining_strategy == 0:    dist_pos, dist_neg = triplet_mining_random(dist_all, mask_pos, mask_neg)\n",
    "    elif param_d.mining_strategy == 1:  dist_pos, dist_neg = triplet_mining_semihard(dist_all, mask_pos, mask_neg, param_d.margin)\n",
    "    else:                               dist_pos, dist_neg = triplet_mining_hard(dist_all, mask_pos, mask_neg, param_d.device)\n",
    "    \n",
    "    loss = F.relu(dist_pos + (param_d.margin - dist_neg))  # calculating triplet loss\n",
    "    \n",
    "    nb1 = torch.sum(dist_pos+param_d.margin < dist_neg).item()\n",
    "    nb2 = torch.sum(loss > 0).item()\n",
    "\n",
    "    #print(f'ok:{nb1}/used:{nb2}/total:{batch_size} \\t loss:{loss.mean().item()}' )\n",
    "    \n",
    "    return loss.mean(), nb1#loss.sum()/nb2, nb1\n",
    "\n",
    "\n",
    "def triplet_mining_random(dist_all, mask_pos, mask_neg):\n",
    "    \"\"\"\n",
    "    Performs online random triplet mining\n",
    "    \"\"\"\n",
    "    # selecting the positive elements of triplets\n",
    "    # we consider each row as an anchor and takes the maximum of the masked row (mask_pos) as the positive\n",
    "    _, sel_pos = torch.max(mask_pos.float() + torch.rand_like(dist_all), dim=1)\n",
    "    dists_pos = torch.gather(input=dist_all, dim=1, index=sel_pos.view(-1, 1))\n",
    "    \n",
    "    # selecting the negative elements of triplets\n",
    "    # we consider each row as an anchor and takes the maximum of the masked row (mask_neg) as the negative\n",
    "    _, sel_neg = torch.max(mask_neg.float() + torch.rand_like(dist_all), dim=1)\n",
    "    dists_neg = torch.gather(input=dist_all, dim=1, index=sel_neg.view(-1, 1))\n",
    "\n",
    "    return dists_pos, dists_neg\n",
    "\n",
    "\n",
    "def triplet_mining_semihard(dist_all, mask_pos, mask_neg, margin):\n",
    "    \"\"\"\n",
    "    Performs online semi-hard triplet mining (a random positive, a semi-hard negative)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- the code below seems wrong\n",
    "    # --- need criteria\n",
    "    # 1) should be negative (should be from a different work-id)\n",
    "    # 2) should be P < N < P+margin\n",
    "\n",
    "    # selecting the positive elements of triplets\n",
    "    # we consider each row as an anchor and takes the maximum of the masked row (mask_pos) as the positive\n",
    "    _, sel_pos = torch.max(mask_pos.float() + torch.rand_like(dist_all), dim=1)\n",
    "    dists_pos = torch.gather(input=dist_all, dim=1, index=sel_pos.view(-1, 1))\n",
    "    \n",
    "    # selecting the negative elements of triplets\n",
    "    _, sel_neg = torch.max( \n",
    "                            (mask_neg + mask_neg * (dist_all < (dists_pos.expand_as(dist_all)).long()+margin)).float() \n",
    "                           + torch.rand_like(dist_all), \n",
    "                           dim=1)\n",
    "\n",
    "    dists_neg = torch.gather(input=dist_all, dim=1, index=sel_neg.view(-1, 1))\n",
    "\n",
    "    return dists_pos, dists_neg\n",
    "\n",
    "\n",
    "def triplet_mining_hard(dist_all, mask_pos, mask_neg, device):\n",
    "    \"\"\"\n",
    "    Performs online hard triplet mining (both positive and negative)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- the code below seems wrong\n",
    "    # --- need criteria\n",
    "    # 1) should be negative (from a different work-id)\n",
    "    # 2) should be N < P\n",
    "\n",
    "    # selecting the positive elements of triplets\n",
    "    # --- for each anchor (row) we take the positive with the largest distance\n",
    "    _, sel_pos = torch.max(dist_all * mask_pos.float(), 1)\n",
    "    dists_pos = torch.gather(input=dist_all, dim=1, index=sel_pos.view(-1, 1))\n",
    "\n",
    "    # modifying the negative mask for hard mining (because we will use the min)\n",
    "    # --- if mask_neg==0 then inf   \n",
    "    # --- if mask_neg==1 then 1\n",
    "    true_value = torch.tensor(float('inf'), device=device)\n",
    "    false_value = torch.tensor(1., device=device)\n",
    "    mask_neg = torch.where(mask_neg == 0, true_value, false_value)\n",
    "    # selecting the negative elements of triplets\n",
    "    # --- for each anchor (row) we take the negative with the smallest distance\n",
    "    _, sel_neg = torch.min(dist_all + mask_neg.float(), dim=1)\n",
    "    dists_neg = torch.gather(input=dist_all, dim=1, index=sel_neg.view(-1, 1))\n",
    "\n",
    "    return dists_pos, dists_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using torchlightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-18-10-42-44\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-07-18-10-42-19</strong> at: <a href='https://wandb.ai/adasp/wandb_cover/runs/iq6k1r1r' target=\"_blank\">https://wandb.ai/adasp/wandb_cover/runs/iq6k1r1r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240718_104221-iq6k1r1r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240718_104249-6v5mgwm6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adasp/wandb_cover/runs/6v5mgwm6' target=\"_blank\">2024-07-18-10-42-44</a></strong> to <a href='https://wandb.ai/adasp/wandb_cover' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adasp/wandb_cover' target=\"_blank\">https://wandb.ai/adasp/wandb_cover</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adasp/wandb_cover/runs/6v5mgwm6' target=\"_blank\">https://wandb.ai/adasp/wandb_cover/runs/6v5mgwm6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_config_d = {}\n",
    "project_name = 'wandb_cover'\n",
    "current_datetime = datetime.datetime.now()\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "expe_name = formatted_datetime\n",
    "print(expe_name)\n",
    "WORK_DIR = './'\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project = project_name, name = expe_name, save_dir = WORK_DIR )\n",
    "wandb_logger.experiment.config.update(train_config_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ids/gpeeters/anaconda3/envs/conda_gpeeters_2024/lib/python3.8/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ids/gpeeters/anaconda3/envs/conda_gpeeters_202 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ids/gpeeters/anaconda3/envs/conda_gpeeters_2024/lib/python3.8/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/ids/gpeeters/anaconda3/envs/conda_gpeeters_202 ...\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/ids/gpeeters/anaconda3/envs/conda_gpeeters_2024/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/ids/gpeeters/proj_tuto-ISMIR2024/code/my_model exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | MOVEModel         | 690 K \n",
      "1 | loss  | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------\n",
      "690 K     Trainable params\n",
      "0         Non-trainable params\n",
      "690 K     Total params\n",
      "2.764     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|     | 1/2 [00:00<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ids/gpeeters/anaconda3/envs/conda_gpeeters_2024/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/ids/gpeeters/anaconda3/envs/conda_gpeeters_2024/lib/python3.8/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ids/gpeeters/anaconda3/envs/conda_gpeeters_2024/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/ids/gpeeters/anaconda3/envs/conda_gpeeters_2024/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (19) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 19/19 [00:02<00:00,  8.98it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved. New best score: 0.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 19/19 [00:01<00:00, 11.88it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.064 >= min_delta = 0.0. New best score: 0.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 19/19 [00:02<00:00,  8.93it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.019 >= min_delta = 0.0. New best score: 0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|| 19/19 [00:02<00:00,  9.49it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.078 >= min_delta = 0.0. New best score: 0.618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 19/19 [00:01<00:00, 11.83it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.029 >= min_delta = 0.0. New best score: 0.589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|| 19/19 [00:01<00:00, 11.79it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|| 19/19 [00:01<00:00, 11.69it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|| 19/19 [00:01<00:00, 11.74it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|| 19/19 [00:01<00:00,  9.63it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|| 19/19 [00:01<00:00, 11.35it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 19/19 [00:01<00:00, 11.72it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|| 19/19 [00:01<00:00, 11.65it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|| 19/19 [00:02<00:00,  9.33it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|| 19/19 [00:01<00:00, 11.56it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.023 >= min_delta = 0.0. New best score: 0.491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|| 19/19 [00:01<00:00, 11.35it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|| 19/19 [00:01<00:00, 11.48it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115: 100%|| 19/19 [00:01<00:00, 11.41it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165: 100%|| 19/19 [00:01<00:00, 11.49it/s, v_num=gwm6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric valid_loss did not improve in the last 50 records. Best score: 0.477. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165: 100%|| 19/19 [00:01<00:00, 11.48it/s, v_num=gwm6]\n"
     ]
    }
   ],
   "source": [
    "class CoverLigthing(pl.LightningModule):\n",
    "    def __init__(self, in_model):\n",
    "        super().__init__()\n",
    "        self.model = in_model\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        items, labels = batch\n",
    "        embedding_m = move_model(items.to(param_d.device))\n",
    "        loss, oktriplet = triplet_loss_mining(embedding_m, labels, param_d)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_oktriplet\", oktriplet)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        items, labels = batch\n",
    "        embedding_m = move_model(items.to(param_d.device))\n",
    "        loss, oktriplet = triplet_loss_mining(embedding_m, labels, param_d)\n",
    "        self.log(\"valid_loss\", loss)\n",
    "        self.log(\"valid_oktriplet\", oktriplet)\n",
    "         \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=param_d.lr, momentum=param_d.momentum)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "move_model = MOVEModel(param_d).to(param_d.device)\n",
    "\n",
    "my_lighting = CoverLigthing(move_model)\n",
    "early_stop_callback = EarlyStopping(monitor=\"valid_loss\", patience=50, verbose=True, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor='valid_loss', dirpath='my_model/', filename='best_model', save_top_k=1, mode='min')\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",  logger = wandb_logger, max_epochs = 500, callbacks = [early_stop_callback, checkpoint_callback])\n",
    "trainer.fit(model=my_lighting, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = CoverDataset(hdf5_feat_file, pyjama_annot_file, do_train=False)\n",
    "\n",
    "move_model.to(param_d.device)\n",
    "with torch.no_grad():\n",
    "    move_model.eval()\n",
    "    embed_all_m = torch.tensor([], device=param_d.device)\n",
    "    coverid_l = []\n",
    "    for performanceid in test_set.data_d.keys():\n",
    "        items, coverid = test_set.getitem_by_performanceid(performanceid)\n",
    "        embedding_m = move_model(items.unsqueeze(0).to(param_d.device))\n",
    "        embed_all_m = torch.cat((embed_all_m, embedding_m), dim=0) \n",
    "        coverid_l.append(coverid)\n",
    "        \n",
    "dist_all_m = pairwise_distance_matrix(embed_all_m)\n",
    "if param_d.norm_dist == 1:  dist_all_m /= param_d.emb_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Ranking Metrics\n",
    "\n",
    "https://gist.github.com/bwhite/3726239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_rank: 26.005\n",
      "mean_reciprocal_rank: 0.37922787945073666\n",
      "precision_at_1: 0.285\n",
      "precision_at_5: 0.11\n",
      "precision_at_10: 0.06849999999999999\n",
      "average_precision: 0.06849999999999999\n"
     ]
    }
   ],
   "source": [
    "def F_mean_rank(relevance):\n",
    "    return relevance.nonzero()[0][0]+1\n",
    "\n",
    "def F_mean_reciprocal_rank(relevance):\n",
    "    return 1./ F_mean_rank(relevance)\n",
    "\n",
    "def F_precision_at_k(relevance, k):\n",
    "    return np.mean(relevance[:k] != 0)\n",
    "\n",
    "def F_average_precision(relevance):\n",
    "    out = [F_precision_at_k(relevance, k + 1) for k in range(relevance.size) if relevance[k]]\n",
    "    return np.mean(out)\n",
    "\n",
    "dist_all_np = dist_all_m.cpu().numpy()\n",
    "nb_target = dist_all_np.shape[0]\n",
    "dist_all_np += 1e6*np.eye(nb_target) # --- to prevent detecting the target itself\n",
    "mean_rank_l, mean_reciprocal_rank_l, precision_at_1_l, precision_at_5_l, precision_at_10_l, average_precision_l = [], [], [], [], [], []\n",
    "\n",
    "score_d = {}\n",
    "for key in ['mean_rank', 'mean_reciprocal_rank', 'precision_at_1','precision_at_5', 'precision_at_10', 'average_precision']:\n",
    "    score_d[key] = [] \n",
    "\n",
    "for idx_target in range(nb_target):\n",
    "    relevance = np.asarray([1 if coverid_l[pos]==coverid_l[idx_target] else 0 for pos in np.argsort(dist_all_np[idx_target,:])])    \n",
    "    score_d['mean_rank'].append( F_mean_rank(relevance) )\n",
    "    score_d['mean_reciprocal_rank'].append( F_mean_reciprocal_rank(relevance) )\n",
    "    score_d['precision_at_1'].append( F_precision_at_k(relevance, 1) )\n",
    "    score_d['precision_at_5'].append( F_precision_at_k(relevance, 5) )\n",
    "    score_d['precision_at_10'].append( F_precision_at_k(relevance, 10) )\n",
    "    score_d['average_precision'].append( F_precision_at_k(relevance, 10) )\n",
    "    \n",
    "for key in score_d.keys():\n",
    "    print(f\"{key}: {np.mean(np.asarray(score_d[key]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_gpeeters_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}