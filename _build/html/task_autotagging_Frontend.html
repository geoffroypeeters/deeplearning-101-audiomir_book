

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Auto-Tagging (front-ends) &#8212; Deep Learning 101 for Audio-based MIR</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'task_autotagging_frontend';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Auto-Tagging (SSL)" href="task_autotagging_ssl.html" />
    <link rel="prev" title="Cover Song Identification (CSI)" href="task_coverdetection.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="front.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/wave.png" class="logo__image only-light" alt="Deep Learning 101 for Audio-based MIR - Home"/>
    <script>document.write(`<img src="_static/wave.png" class="logo__image only-dark" alt="Deep Learning 101 for Audio-based MIR - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="front.html">
                    Deep Learning 101 for Audio-based MIR
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="intro.html">Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="intro_dataset.html">Datasets .hdf5/.pyjama</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro_pytorch.html">Pytorch dataset/dataloader</a></li>

<li class="toctree-l2"><a class="reference internal" href="intro_lightining.html">TorchLightning training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notebook.html">Nodebooks in Colab</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="task_musiccontent.html">Music Audio Analysis</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="task_multipitchestimation.html">Multi-Pitch-Estimation (MPE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_coverdetection.html">Cover Song Identification (CSI)</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Auto-Tagging (front-ends)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_autotagging_ssl.html">Auto-Tagging (SSL)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musicprocessing.html">Music Audio Processing</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_sourceseparation.html">Source Separation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musicgeneration.html">Musical Audio Generation</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_basics.html">Basics of Generative Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_early.html">Early Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_auto.html">Autoregressive Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_diff.html">Generation with Latent Diffusion</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Bricks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bricks_input.html">Inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_frontend.html">Front-ends</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_projection.html">Projections</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_bottleneck.html">Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_architecture.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_paradigm.html">Paradigms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="biography.html">About the authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibiography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Ftask_autotagging_frontend.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/task_autotagging_frontend.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Auto-Tagging (front-ends)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-auto-tagging">Goal of Auto-Tagging ?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-very-short-history-of-auto-tagging">A very short history of Auto-Tagging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-very-short-history-of-chord-estimation">A very short history of Chord Estimation.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-the-task-evaluated">How is the task evaluated ?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-class">Multi-class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-label">Multi-label</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chord-estimation">Chord Estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-popular-datasets">Some popular datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gtzan">GTZAN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#magna-tag-a-tune">Magna-Tag-A-Tune</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rwc-popular-chord-aist-annotations">RWC-Popular-Chord (AIST-Annotations)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-we-can-solve-it-using-deep-learning">How we can solve it using deep learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#actions">Actions:</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="auto-tagging-front-ends">
<span id="lab-auto-tagging"></span><h1>Auto-Tagging (front-ends)<a class="headerlink" href="#auto-tagging-front-ends" title="Permalink to this heading">#</a></h1>
<section id="goal-of-auto-tagging">
<h2>Goal of Auto-Tagging ?<a class="headerlink" href="#goal-of-auto-tagging" title="Permalink to this heading">#</a></h2>
<p>Music auto-tagging is the task of assigning tags (such as genre, style, moods, instrumentation, chords) to a music track.</p>
<p>Tags can be</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>mutually exclusive (multi-class)</p></th>
<th class="head"><p>non-mutually exclusive (multi-label)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Global in time</strong></p></td>
<td><p>Music-genre</p></td>
<td><p>User-tags</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Time-based</strong></p></td>
<td><p>Chord-segments</p></td>
<td><p>Instrument-segments</p></td>
</tr>
</tbody>
</table>
<p><img alt="flow_autotagging" src="_images/flow_autotagging.png" /></p>
<section id="a-very-short-history-of-auto-tagging">
<h3>A very short history of Auto-Tagging<a class="headerlink" href="#a-very-short-history-of-auto-tagging" title="Permalink to this heading">#</a></h3>
<p>The task has a long history in MIR.</p>
<ul class="simple">
<li><p>As soon as <mark>2002 Tzanetakis</mark> et al. <span id="id1">[<a class="reference internal" href="bibiography.html#id64" title="George Tzanetakis and Perry R. Cook. Musical genre classification of audio signals. IEEE Trans. Speech Audio Process., 10(5):293–302, 2002. URL: https://doi.org/10.1109/TSA.2002.800560, doi:10.1109/TSA.2002.800560.">TC02</a>]</span> demonstrated that it is possible to estimate the <code class="docutils literal notranslate"><span class="pre">genre</span></code> using a set of low-level (hand-crafted) audio features (such as MFCC) and simple machine-learning models (such as Gaussian-Mixture-Models).</p></li>
<li><p>Over years, the considered <mark>audio features</mark> improved  <span id="id2">[<a class="reference internal" href="bibiography.html#id35" title="Geoffroy Peeters. A large set of audio features for sound description (similarity and classification) in the cuidado project. Cuidado Project Report, Ircam, 2004.">Pee04</a>]</span>, including block-features <span id="id3">[<a class="reference internal" href="bibiography.html#id34" title="Klaus Seyerlehner. Content-Based Music Recommender Systems: Beyond simple Frame-Level Audio Similarity. PhD thesis, Johannes Kepler Universität, Linz, Austria, December 2010.">Sey10</a>]</span> or speech-inspired features (Universal-Background-Models and Super-Vector <span id="id4">[<a class="reference internal" href="bibiography.html#id33" title="Christophe Charbuillet, Damien Tardieu, and Geoffroy Peeters. Gmm supervector for content based music similarity. In Proc. of DAFx (International Conference on Digital Audio Effects), 425-428. Paris, France, September 2011.">CTP11</a>]</span>), as well as the <mark>machine-learning</mark> models (moving to Random forest or Support-Vector-Machine).</p></li>
<li><p>It also quickly appeared that the <mark>same feature/ML system could be trained to solve many tasks</mark> of tagging or segmentation (genre, mood, speech/music) <span id="id5">[<a class="reference internal" href="bibiography.html#id32" title="Geoffroy Peeters. A generic system for audio indexing: application to speech/ music segmentation and music genre. In In Proc. of DAFx (International Conference on Digital Audio Effects). Bordeaux, France, 2007.">Pee07</a>]</span>, <span id="id6">[<a class="reference internal" href="bibiography.html#id31" title="Juan-Jo Burred and Geoffroy Peeters. An adaptive system for music classification and tagging. In Proc. of LSAS (International Workshop on Learning the Semantics of Audio Signals). Graz, Austria, 2009.">BP09</a>]</span>.</p></li>
</ul>
<p><strong>Deep learning era.</strong></p>
<ul class="simple">
<li><p>We start the story with <mark>Dieleman</mark> <span id="id7">[<a class="reference internal" href="bibiography.html#id73" title="Sander Dieleman. Recommending music on spotify with deep learning. Technical Report, Spotify, http://benanne.github.io/2014/08/05/spotify-cnns.html, 2014.">Die14</a>]</span> who proposes to use a <mark>Conv2d</mark> applied to a <mark>Log-Mel-Spectrogram</mark> with kernel extending over the whole frequency range, therefore performing only convolution over time.<br />
<em>The rational for this, is that, as opposed to natural images, sources in a T/F representation are not invariant by translation over frequencies and the adjacent frequencies are not necesseraly correlated (spacing between harmonics).</em></p></li>
<li><p>Despite this, Choi et al. <span id="id8">[<a class="reference internal" href="bibiography.html#id63" title="Keunwoo Choi, György Fazekas, and Mark B. Sandler. Automatic tagging using deep convolutional neural networks. In Michael I. Mandel, Johanna Devaney, Douglas Turnbull, and George Tzanetakis, editors, Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016, New York City, United States, August 7-11, 2016, 805–811. 2016. URL: https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/009\_Paper.pdf.">CFS16</a>]</span> proposed (with success) to apply Computer Vision <mark>VGG-like architecture</mark> to a time-frequency representation.</p></li>
<li><p>Later on, Pons et al. <span id="id9">[<a class="reference internal" href="bibiography.html#id26" title="Jordi Pons, Thomas Lidy, and Xavier Serra. Experimenting with musically motivated convolutional neural networks. In 14th International Workshop on Content-Based Multimedia Indexing, CBMI 2016, Bucharest, Romania, June 15-17, 2016, 1–6. IEEE, 2016. URL: https://doi.org/10.1109/CBMI.2016.7500246, doi:10.1109/CBMI.2016.7500246.">PLS16</a>]</span> proposed to <mark>design kernel shapes using musical consideration</mark> (with kernel extending over frequencies to represent timbre, over time to represent rhythm).</p></li>
<li><p>In order to avoid having to choose the kernel shape and STFT parameters, it is been proposed to use directly the <mark>audio waveform</mark> as input, the “End-to-End” systems of Dieleman et al. <span id="id10">[<a class="reference internal" href="bibiography.html#id24" title="Sander Dieleman and Benjamin Schrauwen. End-to-end learning for music audio. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2014, Florence, Italy, May 4-9, 2014, 6964–6968. IEEE, 2014. URL: https://doi.org/10.1109/ICASSP.2014.6854950, doi:10.1109/ICASSP.2014.6854950.">DS14</a>]</span> or Lee et al. <span id="id11">[<a class="reference internal" href="bibiography.html#id23" title="Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim, and Juhan Nam. Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms. CoRR, 2017. URL: http://arxiv.org/abs/1703.01789, arXiv:1703.01789.">LPKN17</a>]</span>.</p></li>
<li><p>The task of auto-tagging has also close relationship with their equivalent task in Speech.</p></li>
</ul>
<p><mark><em>We will develop here a model developed initially for speaker recognition by Ravanelli et al. <span id="id12">[<a class="reference internal" href="bibiography.html#id90" title="Mirco Ravanelli and Yoshua Bengio. Speaker recognition from raw waveform with sincnet. In 2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018, 1021–1028. IEEE, 2018. URL: https://doi.org/10.1109/SLT.2018.8639585, doi:10.1109/SLT.2018.8639585.">RB18</a>]</span>.</em></mark></p>
<p>The task is still very active <mark>today</mark>.
For example</p>
<ul class="simple">
<li><p>in the <mark>supervised case</mark>,</p>
<ul>
<li><p>MULE <span id="id13">[<a class="reference internal" href="bibiography.html#id28" title="Matthew C. McCallum, Filip Korzeniowski, Sergio Oramas, Fabien Gouyon, and Andreas F. Ehmann. Supervised and unsupervised learning of audio representations for music understanding. In Preeti Rao, Hema A. Murthy, Ajay Srinivasamurthy, Rachel M. Bittner, Rafael Caro Repetto, Masataka Goto, Xavier Serra, and Marius Miron, editors, Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022, 256–263. 2022. URL: https://archives.ismir.net/ismir2022/paper/000030.pdf.">MKO+22</a>]</span> which uses a more sophisticated ConvNet architecture (Short-Fast-Normalizer-Free Net F0) and training paradigm (contrastive learning)</p></li>
<li><p>PaSST <span id="id14">[<a class="reference internal" href="bibiography.html#id27" title="Khaled Koutini, Shahed Masoudian, Florian Schmid, Hamid Eghbal-zadeh, Jan Schlüter, and Gerhard Widmer. Learning general audio representations with large-scale training of patchout audio transformers. In Joseph Turian, Björn W. Schuller, Dorien Herremans, Katrin Kirchhoff, L. Paola Garc\'ıa-Perera, and Philippe Esling, editors, HEAR: Holistic Evaluation of Audio Representations, Virtual Event, December 13-14, 2021, volume 166 of Proceedings of Machine Learning Research, 65–89. PMLR, 2021. URL: https://proceedings.mlr.press/v166/koutini22a.html.">KMS+21</a>]</span> which uses Vit with tokenized (set of patches) spectrograms fed to a Transformer</p></li>
</ul>
</li>
<li><p>in the <mark>Self-Supervised-Learning case</mark></p>
<ul>
<li><p>with the so-called foundation models such as MERT <span id="id15">[<a class="reference internal" href="bibiography.html#id3" title="Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger B. Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, Zili Wang, Yike Guo, and Jie Fu. MERT: acoustic music understanding model with large-scale self-supervised training. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL: https://openreview.net/forum?id=w3YZ9MSlBu.">LYZ+24</a>]</span> (see second part of this tutorial).</p></li>
</ul>
</li>
</ul>
<p>Fore more details, see the very good tutorial
<a class="reference external" href="https://music-classification.github.io/tutorial/landing-page.html">“musical classification”</a>.</p>
</section>
<section id="a-very-short-history-of-chord-estimation">
<h3>A very short history of Chord Estimation.<a class="headerlink" href="#a-very-short-history-of-chord-estimation" title="Permalink to this heading">#</a></h3>
<p>Chord estimation can be considered as a specific tagging application: it involves applying mutually exclusive labels (of chords) over segments of time.<br />
However, it has (at least) two specificities:</p>
<ul class="simple">
<li><p>chord transition follow <mark>musical rules</mark> which can be represented by a <mark>language model</mark>.</p></li>
<li><p>some chord are equivalent, their spelling depends on the choice of the level of detail, and their choice on the</p></li>
</ul>
<p>Therefore, <mark>ASR (Automatic Speech Recognition)</mark> inspired techniques has been developed at first <span id="id16">[<a class="reference internal" href="bibiography.html#id30" title="A. Sheh and Daniel P. W. Ellis. Chord segmentation and recognition using em-trained hidden markov models. In Proc. of ISMIR (International Society for Music Information Retrieval), 183-189. Baltimore, Maryland, USA, 2003.">SE03</a>]</span> or <span id="id17">[<a class="reference internal" href="bibiography.html#id29" title="Hélène Papadopoulos and Geoffroy Peeters. Large-scale study of chord estimation algorithms based on chroma representation. In Proc. of IEEE CBMI (International Workshop on Content-Based Multimedia Indexing). Bordeaux, France, 2007.">PP07</a>]</span> with</p>
<ul class="simple">
<li><p>an <strong>acoustic model</strong> representing <span class="math notranslate nohighlight">\(p(\text{chord}|\text{chroma})\)</span> and</p></li>
<li><p>a <strong>language model</strong>, often a Hidden Markov Model, representing <span class="math notranslate nohighlight">\(p(\text{chord}_{t}|\text{chord}_{t-1}).\)</span></p></li>
</ul>
<p><strong>Deep learning era.</strong>
In the case of chord estimation, deep learning is also now commonly used.
One seminal paper for this is McFee at al. <span id="id18">[<a class="reference internal" href="bibiography.html#id78" title="Brian McFee and Juan Pablo Bello. Structured training for large-vocabulary chord recognition. In Sally Jo Cunningham, Zhiyao Duan, Xiao Hu, and Douglas Turnbull, editors, Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, 188–194. 2017. URL: https://ismir2017.smcnus.org/wp-content/uploads/2017/10/77\_Paper.pdf.">MB17</a>]</span></p>
<ul class="simple">
<li><p>the model is a RCNN (a ConvNet followed by a <mark>bi-directional RNN</mark>, here GRU)</p></li>
<li><p>the model is trained to use an <mark>inner representation</mark> which relates to the <code class="docutils literal notranslate"><span class="pre">root</span></code>, <code class="docutils literal notranslate"><span class="pre">bass</span></code> and <code class="docutils literal notranslate"><span class="pre">pitches</span></code> (the <mark>CREMA</mark>)</p>
<ul>
<li><p>this allows learning representation which brings together close (but different) chords</p></li>
</ul>
</li>
</ul>
<p><mark><em>We will develop here a similar model based on the combination of Conv2d and Bi-LSTM but without the multi-task approach.</em></mark></p>
</section>
</section>
<section id="how-is-the-task-evaluated">
<h2>How is the task evaluated ?<a class="headerlink" href="#how-is-the-task-evaluated" title="Permalink to this heading">#</a></h2>
<p>We consider a set of classes <span class="math notranslate nohighlight">\(c \in \{1,\ldots,C\}\)</span>.</p>
<section id="multi-class">
<h3>Multi-class<a class="headerlink" href="#multi-class" title="Permalink to this heading">#</a></h3>
<p>In a <strong>multi-class</strong> problem, the classes are <strong>mutually exclusive</strong>.</p>
<ul class="simple">
<li><p>The outputs of the (neural network) model <span class="math notranslate nohighlight">\(o_c\)</span> therefore go to a softmax function.</p></li>
<li><p>The outputs of the softmax, <span class="math notranslate nohighlight">\(p_c\)</span>, then represent the probability <span class="math notranslate nohighlight">\(P(Y=c|X)\)</span>.</p></li>
<li><p>The predicted class is then chosen as <span class="math notranslate nohighlight">\(\arg\max_c p_c\)</span>.</p></li>
</ul>
<p>We evaluate the performances by computing the standard</p>
<ul class="simple">
<li><p><mark>Accuracy, Recall, Precision, F-measure</mark> for each class <span class="math notranslate nohighlight">\(c\)</span> and then take the average over classes <span class="math notranslate nohighlight">\(c\)</span>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="n">classification_reports</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">labels_idx</span><span class="p">,</span>
																								<span class="n">labels_pred_idx</span><span class="p">,</span>
																								<span class="n">output_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">labels_idx</span><span class="p">,</span> <span class="n">labels_pred_idx</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="multi-label">
<h3>Multi-label<a class="headerlink" href="#multi-label" title="Permalink to this heading">#</a></h3>
<p>In the <strong>multi-label</strong> problem, the classes are <strong>NOT mutually exclusive</strong>.</p>
<ul class="simple">
<li><p>Each <span class="math notranslate nohighlight">\(o_c\)</span> therefore goes individually to a sigmoid function (multi-label is processed as a set of parallel independent binary classification problems).</p></li>
<li><p>The outputs of the sigmoids <span class="math notranslate nohighlight">\(p_c\)</span> then represent <span class="math notranslate nohighlight">\(P(Y_c=1|X)\)</span>.</p></li>
<li><p>We then need to set a threshold <span class="math notranslate nohighlight">\(\tau\)</span> on each <span class="math notranslate nohighlight">\(p_c\)</span> to decide wether class <span class="math notranslate nohighlight">\(c\)</span> exist or not.</p></li>
</ul>
<p>Using a default threshold (<span class="math notranslate nohighlight">\(\tau=0.5\)</span>) of course allows to use the afore-mentioned metrics (Accuracy, Recall, Precision, F-measure).<br />
However, in practice, <mark>we want to measure the performances independently of the choice of a given threshold</mark>.</p>
<p>This can be using either</p>
<ul class="simple">
<li><p>the <mark><strong>AUC (Area Under the Curve) of the ROC</strong></mark>.
The ROC curve represents the values of TPrate versus FPrate for all possible choices of a threshold <span class="math notranslate nohighlight">\(\tau\)</span>.
The larger the AUC-ROC is (maximum of 1) the more discrimination is between the Positive and Negative classes.
A value of 0.5 indicates no discrimination (random system).</p></li>
<li><p>the <mark><strong>mean-Average-Precision (mAP)</strong></mark>.
The mAP measures the AUC of the Precision versus Recall curve for all possible choices of a threshold <span class="math notranslate nohighlight">\(\tau)\)</span>.</p></li>
</ul>
<p>The AUC-ROC is known to be sensitive to class imbalancing (in case of multi-label, negative examples are usually more numerous than positive ones, hence the FPrate is artificially low leading to good AUC of ROC).
In the opposite, mAP which relies on the Precision is less sensitive to class imbalancing and is therefoe prefered.</p>
<p><img alt="AUC-ROC-MAP" src="_images/brick_roc_map_P.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">average_precision_score</span>
<span class="n">roc_auc_score</span><span class="p">(</span><span class="n">labels_idx</span><span class="p">,</span> <span class="n">labels_pred_prob</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">)</span>
<span class="n">average_precision_score</span><span class="p">(</span><span class="n">labels_idx</span><span class="p">,</span> <span class="n">labels_pred_prob</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>About the averages in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">macro</span></code> <strong>average</strong>: computes the metric independently for each class and then takes the average (i.e., all classes are treated equally, regardless of their frequency).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">micro</span></code> <strong>average</strong>: aggregates the contributions of all classes before calculating the overall metric, essentially treating the problem as a single binary classification task across all samples</p></li>
</ul>
</section>
<section id="chord-estimation">
<h3>Chord Estimation<a class="headerlink" href="#chord-estimation" title="Permalink to this heading">#</a></h3>
<p>In the following (for the sake of simplicity) we will evaluate our chord estimation system <mark>as a multi-class problem</mark>.</p>
<p>However, chord are not simple labels.
Indeed, chord annotation is partly subjective, some chords are equivalent, and the spelling of a chord depends on the choice of the level of detail (the choice of a dictionary).<br />
For this reason, <code class="docutils literal notranslate"><span class="pre">mir_eval</span></code> <span id="id19">[<a class="reference internal" href="bibiography.html#id22" title="Colin Raffel, Brian McFee, Eric J. Humphrey, Justin Salamon, Oriol Nieto, Dawen Liang, and Daniel P. W. Ellis. Mir_eval: A transparent implementation of common MIR metrics. In Hsin-Min Wang, Yi-Hsuan Yang, and Jin Ha Lee, editors, Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014, Taipei, Taiwan, October 27-31, 2014, 367–372. 2014. URL: http://www.terasoft.com.tw/conf/ismir2014/proceedings/T066\_320\_Paper.pdf.">RMH+14</a>]</span> or Pauwels et al. <span id="id20">[<a class="reference internal" href="bibiography.html#id25" title="Johan Pauwels and Geoffroy Peeters. Evaluating automatically estimated chord sequences. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, 749–753. IEEE, 2013. URL: https://doi.org/10.1109/ICASSP.2013.6637748, doi:10.1109/ICASSP.2013.6637748.">PP13</a>]</span> proposed metrics that allows measuring the correctness of the <code class="docutils literal notranslate"><span class="pre">root</span></code>, the <code class="docutils literal notranslate"><span class="pre">major/minor</span></code> component, the <code class="docutils literal notranslate"><span class="pre">bass</span></code> or the constitution in terms of <code class="docutils literal notranslate"><span class="pre">chroma</span></code>.</p>
</section>
</section>
<section id="some-popular-datasets">
<h2>Some popular datasets<a class="headerlink" href="#some-popular-datasets" title="Permalink to this heading">#</a></h2>
<p>A (close to) exhaustive list of MIR datasets is available in the <a class="reference external" href="https://ismir.net/resources/datasets/">ismir.net web site</a>.</p>
<p>We have chosen the following ones since they are often used, they represent the multi-class, multi-label and chord estimation problems, and their audio is easely accessible.</p>
<section id="gtzan">
<h3>GTZAN<a class="headerlink" href="#gtzan" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="http://marsyas.info/downloads/datasets.html">GTZAN</a> contains 1000 audio files of 30s duration, each with a single (<strong>multi-class</strong>) genre label</p>
<ul class="simple">
<li><p>among 10 classes: ‘blues’,’classical’,’country’,’disco’,’hiphop’,’jazz’,’metal’,’pop’, ‘reggae’,’rock’</p></li>
</ul>
<p>Note that GTZAN has been criticized for the quality of its genre label <span id="id21">[<a class="reference internal" href="bibiography.html#id2" title="Bob L. Sturm. The GTZAN dataset: its contents, its faults, their effects on evaluation, and its future use. CoRR, 2013. URL: http://arxiv.org/abs/1306.1461, arXiv:1306.1461.">Stu13</a>]</span>; so results should be considered with cares.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;entry&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;filepath&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;blues+++blues.00000.wav&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;genre&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;blues&quot;</span><span class="p">}</span>
                <span class="p">]</span>
            <span class="p">},</span>
            <span class="p">{</span>
                <span class="s2">&quot;filepath&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;blues+++blues.00001.wav&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;genre&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;blues&quot;</span><span class="p">}</span>
                <span class="p">]</span>
            <span class="p">}</span>
          <span class="p">]</span>
</pre></div>
</div>
</section>
<section id="magna-tag-a-tune">
<h3>Magna-Tag-A-Tune<a class="headerlink" href="#magna-tag-a-tune" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset">Magna-Tag-A-Tune (MTT)</a> is a <strong>multi-label</strong> large-scale dataset of 25,000 30-second music clips from various genres, each annotated with</p>
<ul class="simple">
<li><p>multiple tags describing genre, mood, instrumentation, and other musical attributes such as (‘guitar’, ‘classical’, ‘slow’, ‘techno’, ‘strings’, ‘drums’, ‘electronic’, ‘rock’, ‘fast’, ‘piano’, …)</p></li>
</ul>
<p>We only use a subset of this dataset by only selecting the most 50 used tags and further reducing the number of audio by 20.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;entry&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;filepath&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;0+++american_bach_soloists-j_s__bach__cantatas_volume_v-01-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_i_sinfonia-117-146.mp3&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;tag&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;classical&quot;</span><span class="p">},</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;violin&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;artist&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;American Bach Soloists&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;album&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;J.S. Bach - Cantatas Volume V&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;track_number&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;Gleichwie der Regen und Schnee vom Himmel fallt BWV 18_ I Sinfonia&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;clip_id&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="mi">29</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;original_url&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;http://he3.magnatune.com/all/01--Gleichwie</span><span class="si">%20d</span><span class="s2">er%20Regen</span><span class="si">%20u</span><span class="s2">nd%20Schnee%20vom%20Himmel</span><span class="si">%20f</span><span class="s2">allt%20BWV%2018_%20I%20Sinfonia--ABS.mp3&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;segmentEnd&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="mi">146</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;segmentStart&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="mi">117</span><span class="p">}</span>
                <span class="p">]</span>
            <span class="p">},</span>
          <span class="p">]</span>
</pre></div>
</div>
</section>
<section id="rwc-popular-chord-aist-annotations">
<h3>RWC-Popular-Chord (AIST-Annotations)<a class="headerlink" href="#rwc-popular-chord-aist-annotations" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://staff.aist.go.jp/m.goto/RWC-MDB/AIST-Annotation/">RWC-Popular-Chord (AIST-Annotations)</a><span id="id22">[<a class="reference internal" href="bibiography.html#id40" title="Masataka Goto, Hiroki Hashiguchi, Takuichi Nishimura, and Ryuichi Oka. RWC music database: popular, classical and jazz music databases. In ISMIR 2002, 3rd International Conference on Music Information Retrieval, Paris, France, October 13-17, 2002, Proceedings. 2002. URL: http://ismir2002.ismir.net/proceedings/03-SP04-1.pdf.">GHNO02</a>]</span>, <span id="id23">[<a class="reference internal" href="bibiography.html#id41" title="Masataka Goto. AIST annotation for the RWC music database. In ISMIR 2006, 7th International Conference on Music Information Retrieval, Victoria, Canada, 8-12 October 2006, Proceedings, 359–360. 2006.">Got06</a>]</span> is one of the earliest and remains one of the most comprehensive datasets, featuring annotations for genre, structure, beat, chords, and multiple pitches. We use the subset of tracks named <code class="docutils literal notranslate"><span class="pre">Popular-Music-Dataset</span></code>. <br />
<em>This dataset has been made available online with Masataka Goto’s permission specifically for this tutorial. For any other use, please contact Masataka Goto to obtain authorization.</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;entry&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;filepath&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;001&quot;</span><span class="p">}</span>
                <span class="p">],</span>
                <span class="s2">&quot;chord&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;N:-&quot;</span><span class="p">,</span> <span class="s2">&quot;time&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">0.104</span><span class="p">},</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;G#:min&quot;</span><span class="p">,</span> <span class="s2">&quot;time&quot;</span><span class="p">:</span> <span class="mf">0.104</span><span class="p">,</span> <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">1.754</span><span class="p">},</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;F#:maj&quot;</span><span class="p">,</span> <span class="s2">&quot;time&quot;</span><span class="p">:</span> <span class="mf">1.858</span><span class="p">,</span><span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">1.7879999999999998</span><span class="p">},</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;E:maj&quot;</span><span class="p">,</span><span class="s2">&quot;time&quot;</span><span class="p">:</span> <span class="mf">3.646</span><span class="p">,</span><span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">1.7409999999999997</span><span class="p">},</span>
                    <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;F#:maj&quot;</span><span class="p">,</span> <span class="s2">&quot;time&quot;</span><span class="p">:</span> <span class="mf">5.387</span><span class="p">,</span> <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">3.6800000000000006</span><span class="p">},</span>
                <span class="p">]</span>
            <span class="p">}</span>
          <span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="how-we-can-solve-it-using-deep-learning">
<h2>How we can solve it using deep learning<a class="headerlink" href="#how-we-can-solve-it-using-deep-learning" title="Permalink to this heading">#</a></h2>
<p>Our goal is to show that we can <mark>solve the three tasks</mark> (multi-class GTZAN, multi-label MTT and chord estimation RWC-Pop) with a <mark>single code</mark>.
Depending on the task, we of course adapt the model (defined in the <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> files).</p>
<p><mark>multi-class/multi-label</mark>:</p>
<ul class="simple">
<li><p>GTZAN and RWC-Pop-Chord are <strong>multi-class</strong> problems <span class="math notranslate nohighlight">\(\Rightarrow\)</span> softmax and categorial-CE</p></li>
<li><p>MTT is <strong>multi-label</strong> <span class="math notranslate nohighlight">\(\Rightarrow\)</span> sigmoids and BCEs</p></li>
</ul>
<p><mark>global/local</mark>:</p>
<ul class="simple">
<li><p>GTZAN and MTT have <strong>global</strong> annotations <span class="math notranslate nohighlight">\(\Rightarrow\)</span> we reduce the time axis using AutoPoolWeightSplit</p></li>
<li><p>RWC-Pop-Chord have <strong>local</strong> annotations with a language model <span class="math notranslate nohighlight">\(\Rightarrow\)</span> we use a bi-LSTM.</p></li>
</ul>
<p>For GTZAN and MTT our core model is the <mark>SincNet model</mark> illustrated below.</p>
<p><img alt="sincnet" src="_images/brick_sincnet.png" /><br />
<strong>Figure</strong>. <em>SincNet model. image source: SincNet <span id="id24">[<a class="reference internal" href="bibiography.html#id90" title="Mirco Ravanelli and Yoshua Bengio. Speaker recognition from raw waveform with sincnet. In 2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018, 1021–1028. IEEE, 2018. URL: https://doi.org/10.1109/SLT.2018.8639585, doi:10.1109/SLT.2018.8639585.">RB18</a>]</span></em></p>
<p>We will vary in turn</p>
<ul class="simple">
<li><p>the <strong>inputs</strong>: <a class="reference internal" href="bricks_input.html#lab-waveform"><span class="std std-ref">waveform</span></a>, <a class="reference internal" href="bricks_input.html#lab-lms"><span class="std std-ref">Log-Mel-Spectrogram</span></a> or <a class="reference internal" href="bricks_input.html#lab-cqt"><span class="std std-ref">CQT</span></a></p></li>
<li><p>the <strong>front-end</strong>:</p>
<ul>
<li><p><a class="reference internal" href="bricks_projection.html#lab-conv2d"><span class="std std-ref">Conv-2d</span></a> when the input is LMS or CQT</p></li>
<li><p><a class="reference internal" href="bricks_frontend.html#lab-sincnet"><span class="std std-ref">SincNet</span></a>, <a class="reference internal" href="bricks_frontend.html#lab-conv1d"><span class="std std-ref">Conv-1D</span></a> or <a class="reference internal" href="bricks_frontend.html#lab-tcn"><span class="std std-ref">TCN</span></a> when the input is waveform</p></li>
</ul>
</li>
<li><p>the model <strong>blocks</strong>:</p>
<ul>
<li><p><a class="reference internal" href="bricks_frontend.html#lab-conv1d"><span class="std std-ref">Conv-1d</span></a>, Linear and <a class="reference internal" href="bricks_architecture.html#lab-autopoolweightsplit"><span class="std std-ref">AutoPoolWeightSplit</span></a> for multi-class, multi-label</p></li>
<li><p><a class="reference internal" href="bricks_frontend.html#lab-conv1d"><span class="std std-ref">Conv-1d</span></a>, Linear and <a class="reference internal" href="bricks_architecture.html#lab-rnn"><span class="std std-ref">RNN/LSTM</span></a> for segment (chord over time)</p></li>
</ul>
</li>
</ul>
<p><img alt="expe" src="_images/expe_autotagging_P.png" /></p>
<section id="experiments">
<h3>Experiments:<a class="headerlink" href="#experiments" title="Permalink to this heading">#</a></h3>
<p>The code is available here:</p>
<ul class="simple">
<li><p>(Main notebook)(<a class="github reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Auto_Tagging.ipynb">geoffroypeeters/deeplearning-101-audiomir_notebook</a>)</p></li>
<li><p>(Config Auto-Tagging)[<a class="github reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/config_autotagging.yaml">geoffroypeeters/deeplearning-101-audiomir_notebook</a>]</p></li>
<li><p>(Config Chord)[<a class="github reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/config_chord.yaml">geoffroypeeters/deeplearning-101-audiomir_notebook</a>]</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-left head"><p>Dataset</p></th>
<th class="text-left head"><p>Input</p></th>
<th class="text-left head"><p>Frontend</p></th>
<th class="text-left head"><p>Model</p></th>
<th class="text-left head"><p>Results</p></th>
<th class="text-left head"><p>Code</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>GTZAN</p></td>
<td class="text-left"><p>LMS</p></td>
<td class="text-left"><p>Conv2d(128,5)</p></td>
<td class="text-left"><p>Conv1d/Linear/AutoPoolWeightSplit</p></td>
<td class="text-left"><p>macroRecall: 0.56</p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Auto_Tagging.ipynb_D1-I1-C1.ipynb">LINK</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>GTZAN</p></td>
<td class="text-left"><p>Waveform</p></td>
<td class="text-left"><p>SincNet/Abs</p></td>
<td class="text-left"><p>Conv1d/Linear/AutoPoolWeightSplit</p></td>
<td class="text-left"><p>macroRecall: 0.56</p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Auto_Tagging.ipynb_D1-I2-C2.ipynb">LINK</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>GTZAN</p></td>
<td class="text-left"><p>Waveform</p></td>
<td class="text-left"><p>Conv1D</p></td>
<td class="text-left"><p>Conv1d/Linear/AutoPoolWeightSplit</p></td>
<td class="text-left"><p>macroRecall: 0.54</p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Auto_Tagging.ipynb_D1-I2-C3.ipynb">LINK</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>GTZAN</p></td>
<td class="text-left"><p>Waveform</p></td>
<td class="text-left"><p>TCN</p></td>
<td class="text-left"><p>Conv1d/Linear/AutoPoolWeightSplit</p></td>
<td class="text-left"><p>macroRecall: 0.46</p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Auto_Tagging.ipynb_D1-I2-C4.ipynb">LINK</a></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>MTT</p></td>
<td class="text-left"><p>LMS</p></td>
<td class="text-left"><p>Conv2d(128,5)</p></td>
<td class="text-left"><p>Conv1d/Linear/AutoPoolWeightSplit</p></td>
<td class="text-left"><p>AUC: 0.81, avgPrec: 0.29</p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Auto_Tagging.ipynb_D2-I1-C1.ipynb">LINK</a></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>RWC-Pop-Chord</p></td>
<td class="text-left"><p>CQT</p></td>
<td class="text-left"><p>Conv2D(1,5)(5,1)*</p></td>
<td class="text-left"><p>Conv1D/LSTM/Linear</p></td>
<td class="text-left"><p>macroRecall: 0.54</p></td>
<td class="text-left"><p><a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Auto_Tagging.ipynb_D3-I3-Chord.ipynb">LINK</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="actions">
<h3>Actions:<a class="headerlink" href="#actions" title="Permalink to this heading">#</a></h3>
<p>We show that</p>
<ul class="simple">
<li><p>autotagging config file</p></li>
<li><p>multi-class: results, CM and Tag-O-Gram</p></li>
<li><p>multi-class:: learned filters SincNet, code SincNet</p></li>
<li><p>multi-class: learned filters Conv1d</p></li>
<li><p>multi-label: results, tag-o-gram:</p></li>
<li><p>chord config file</p></li>
<li><p>chord: training patches</p></li>
<li><p>chord: resutls, tag-o-gram</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="task_coverdetection.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Cover Song Identification (CSI)</p>
      </div>
    </a>
    <a class="right-next"
       href="task_autotagging_ssl.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Auto-Tagging (SSL)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-auto-tagging">Goal of Auto-Tagging ?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-very-short-history-of-auto-tagging">A very short history of Auto-Tagging</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-very-short-history-of-chord-estimation">A very short history of Chord Estimation.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-the-task-evaluated">How is the task evaluated ?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-class">Multi-class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-label">Multi-label</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chord-estimation">Chord Estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-popular-datasets">Some popular datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gtzan">GTZAN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#magna-tag-a-tune">Magna-Tag-A-Tune</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rwc-popular-chord-aist-annotations">RWC-Popular-Chord (AIST-Annotations)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-we-can-solve-it-using-deep-learning">How we can solve it using deep learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#actions">Actions:</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Geoffroy Peeters, Gabriel Meseguer-Brocal, Alain Riou, Stefan Lattner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>