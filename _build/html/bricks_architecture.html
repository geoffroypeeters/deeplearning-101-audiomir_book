
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Architectures &#8212; Deep Learning 101 for Audio-based MIR</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'bricks_architecture';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Paradigms" href="bricks_paradigm.html" />
    <link rel="prev" title="Bottleneck" href="bricks_bottleneck.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="front.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/wave.png" class="logo__image only-light" alt="Deep Learning 101 for Audio-based MIR - Home"/>
    <script>document.write(`<img src="_static/wave.png" class="logo__image only-dark" alt="Deep Learning 101 for Audio-based MIR - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="front.html">
                    Deep Learning 101 for Audio-based MIR
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="task_autotagging_frontend.html">Auto-Tagging-FrontEnd</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_multipitchestimation.html">Multi-Pitch-Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_coverdetection.html">Cover Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_sourceseparation.html">Source Separation</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_autotagging_ssl.html">Auto-Tagging-SSL</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_musicgeneration.html">Musical Audio Generation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Bricks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bricks_input.html">Inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_frontend.html">Front-ends</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_projection.html">Projections</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_bottleneck.html">Bottleneck</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_paradigm.html">Paradigms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="biography.html">About the authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibiography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fbricks_architecture.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/bricks_architecture.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Architectures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#u-net">U-Net</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#many-to-one-reducing-the-time-dimensions">Many to One: reducing the time dimensions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-weighting">Attention weighting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-pool">Auto-Pool</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-models">Using models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-lstm">RNN/ LSTM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-self-attention">Transformer/ Self-Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conformer">Conformer</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="architectures">
<h1>Architectures<a class="headerlink" href="#architectures" title="Link to this heading">#</a></h1>
<section id="u-net">
<span id="lab-unet"></span><h2>U-Net<a class="headerlink" href="#u-net" title="Link to this heading">#</a></h2>
<p>The U-Net was proposed in <span id="id1">[<a class="reference internal" href="bibiography.html#id20" title="Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells III, and Alejandro F. Frangi, editors, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, volume 9351 of Lecture Notes in Computer Science, 234–241. Springer, 2015. URL: https://doi.org/10.1007/978-3-319-24574-4\_28, doi:10.1007/978-3-319-24574-4\_28.">RFB15</a>]</span> in the framework of biomedical image segmentation and made popular in MIR by <span id="id2">[<a class="reference internal" href="bibiography.html#id19" title="Andreas Jansson, Eric J. Humphrey, Nicola Montecchio, Rachel M. Bittner, Aparna Kumar, and Tillman Weyde. Singing voice separation with deep u-net convolutional networks. In Sally Jo Cunningham, Zhiyao Duan, Xiao Hu, and Douglas Turnbull, editors, Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, 745–751. 2017. URL: https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171\_Paper.pdf.">JHM+17</a>]</span> for singing voice separation.</p>
<p>The U-Net is an auto-encoder with skip-connections.
The encoder (left part) downsample the spatial dimensions and increase the depth, while the decoder (right part) upsample the spatial dimensions and decrease the depth.
Skip connections are added between equivalent layers of the encoder and decoder: the 256 channels level of the encoder is concatenated with the 256 level of the decoder to form a 512 tensor.</p>
<p><img alt="brick_unet" src="_images/brick_unet.png" /></p>
<p><em>image source: <span id="id3">[<a class="reference internal" href="bibiography.html#id20" title="Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells III, and Alejandro F. Frangi, editors, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, volume 9351 of Lecture Notes in Computer Science, 234–241. Springer, 2015. URL: https://doi.org/10.1007/978-3-319-24574-4\_28, doi:10.1007/978-3-319-24574-4\_28.">RFB15</a>]</span></em></p>
<p>The goal of the skip-connections are two-folds:</p>
<ul class="simple">
<li><p>to bring back details of the original images to the decoder (the bottleneck being to compressed to represent detailed information)</p></li>
<li><p>to facilitate the backpropagation of the gradient.</p></li>
</ul>
<p>The upsampling part can be done either</p>
<ul class="simple">
<li><p>using Transposed Convolution (hence a well-known checkerboard artefact may appears)</p></li>
<li><p>using Interpolation followed by Normal convolution</p></li>
</ul>
</section>
<section id="many-to-one-reducing-the-time-dimensions">
<h2>Many to One: reducing the time dimensions<a class="headerlink" href="#many-to-one-reducing-the-time-dimensions" title="Link to this heading">#</a></h2>
<p>They are many different ways to reduce map a temporel sequence of embeddings <span class="math notranslate nohighlight">\(\{X_1, \ldots X_{T_x}\}\)</span>(Many) to a single embedding <span class="math notranslate nohighlight">\(X\)</span> (One).</p>
<p>Such a mechanism can be necessary in order to map the temporel embedding provided by the last layer of a network to a single ground-truth (such as in auto-tagging, where the whole track is from a given genre, or in Acoustic Scene Classification).</p>
<p><img alt="brick_pooling" src="_images/brick_pooling.png" /></p>
<p>The most simple way to achieve this is to use the Mean/Average value (Average Pooling) or Maximum value (Max Pooling) of the <span class="math notranslate nohighlight">\(X_t\)</span> over time (as done for example in <span id="id4">[<a class="reference internal" href="bibiography.html#id25">Die14</a>]</span>).</p>
<section id="attention-weighting">
<h3>Attention weighting<a class="headerlink" href="#attention-weighting" title="Link to this heading">#</a></h3>
<p>Another possibility is to compute a weighted sum of the values <span class="math notranslate nohighlight">\(X_t\)</span> where the weights <span class="math notranslate nohighlight">\(a_t\)</span> are attention parameters:
<span class="math notranslate nohighlight">\(X = \sum_{t=0}^{T_x-1} a_t X_t\)</span></p>
<p>In <span id="id5">[<a class="reference internal" href="bibiography.html#id23" title="Siddharth Gururani, Mohit Sharma, and Alexander Lerch. An attention mechanism for musical instrument recognition. In Arthur Flexer, Geoffroy Peeters, Julián Urbano, and Anja Volk, editors, Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands, November 4-8, 2019, 83–90. 2019. URL: http://archives.ismir.net/ismir2019/paper/000007.pdf.">GSL19</a>]</span>, it is proposed to compute these weights <span class="math notranslate nohighlight">\(a_t\)</span> either</p>
<ul class="simple">
<li><p>by computing a new projection of the <span class="math notranslate nohighlight">\(X_t\)</span> and then normalizing them:
<span class="math notranslate nohighlight">\(a_t = \frac{\sigma(v^T h(X_t))}{\sum_{\tau} \sigma(v^T h(X_{\tau}))}\)</span></p></li>
<li><p>doing the same after splitting <span class="math notranslate nohighlight">\(X_t\)</span> in two (along the channel dimensions): the first part being used to compute “values”, the second to compute “weights”</p></li>
</ul>
<p><img alt="brick_attention_instrument" src="_images/brick_attention_instrument.png" /></p>
<p><em>image source: <span id="id6">[<a class="reference internal" href="bibiography.html#id23" title="Siddharth Gururani, Mohit Sharma, and Alexander Lerch. An attention mechanism for musical instrument recognition. In Arthur Flexer, Geoffroy Peeters, Julián Urbano, and Anja Volk, editors, Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands, November 4-8, 2019, 83–90. 2019. URL: http://archives.ismir.net/ismir2019/paper/000007.pdf.">GSL19</a>]</span></em></p>
</section>
<section id="auto-pool">
<h3>Auto-Pool<a class="headerlink" href="#auto-pool" title="Link to this heading">#</a></h3>
<p>The above attention mechanism can by combined with the auto-pool operators proposed by <span id="id7">[<a class="reference internal" href="bibiography.html#id24" title="Brian McFee, Justin Salamon, and Juan Pablo Bello. Adaptive pooling operators for weakly labeled sound event detection. IEEE ACM Trans. Audio Speech Lang. Process., 26(11):2180–2193, 2018. URL: https://doi.org/10.1109/TASLP.2018.2858559, doi:10.1109/TASLP.2018.2858559.">MSB18</a>]</span>.</p>
<p>The auto-pool operators is defined as <span class="math notranslate nohighlight">\(a_t = \frac{\exp(\alpha X_t)}{\sum_{\tau} \exp(\alpha X_{\tau})}\)</span></p>
<p>It uses a parameter <span class="math notranslate nohighlight">\(\alpha\)</span> which allows to range from <span class="math notranslate nohighlight">\(\alpha=0\)</span> (unweighted, a.k.a. average pooling), <span class="math notranslate nohighlight">\(\alpha=1\)</span> (softmax weighted mean), <span class="math notranslate nohighlight">\(\alpha=\infty\)</span>: (a.k.a. max pooling).
The <span class="math notranslate nohighlight">\(\alpha\)</span> parameters is a trainable parameters (optimized using SGD).</p>
<p><img alt="brick_autopool" src="_images/brick_autopool.png" /></p>
<p><em>image source: <span id="id8">[<a class="reference internal" href="bibiography.html#id24" title="Brian McFee, Justin Salamon, and Juan Pablo Bello. Adaptive pooling operators for weakly labeled sound event detection. IEEE ACM Trans. Audio Speech Lang. Process., 26(11):2180–2193, 2018. URL: https://doi.org/10.1109/TASLP.2018.2858559, doi:10.1109/TASLP.2018.2858559.">MSB18</a>]</span></em></p>
</section>
<section id="using-models">
<h3>Using models<a class="headerlink" href="#using-models" title="Link to this heading">#</a></h3>
<p>It is also possible to use a <strong>RNN/LSTM in Many-to-One configuration</strong> (only the last hidden state <span class="math notranslate nohighlight">\(X_{T_x}\)</span> is mapped to an output <span class="math notranslate nohighlight">\(\hat{y}\)</span>).</p>
<p>Finally it is possible to add an extra CLASS token to a Transformer architecture.</p>
<p>It should be noted that the term “Attention” encapsultates a large set of different paradigms.</p>
<p>In the <strong>encode-decoder architecture</strong> <span id="id9">[<a class="reference internal" href="bibiography.html#id22" title="Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. 2015. URL: http://arxiv.org/abs/1409.0473.">BCB15</a>]</span> it is used during decoding to define the correct context <span class="math notranslate nohighlight">\(c(\tau)\)</span> to be used to generate the hidden state <span class="math notranslate nohighlight">\(s(\tau)\)</span>. For this it compares <span class="math notranslate nohighlight">\(s(\tau-1)\)</span> to all the hidden state of the encoder <span class="math notranslate nohighlight">\(a(t)\)</span>.</p>
<p>In the <strong>transformer architecture</strong> <span id="id10">[<a class="reference internal" href="bibiography.html#id21" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 5998–6008. 2017. URL: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.">VSP+17</a>]</span> it is used to compute a self-attention. For this, the <span class="math notranslate nohighlight">\(x(t)\)</span> are mapped (using matrix projections) to query <span class="math notranslate nohighlight">\(q(t)\)</span>, key <span class="math notranslate nohighlight">\(k(t)\)</span> and values <span class="math notranslate nohighlight">\(v(t)\)</span>. A given <span class="math notranslate nohighlight">\(q(\tau)\)</span> is then compared to all <span class="math notranslate nohighlight">\(k(t)\)</span> to compute attention weights <span class="math notranslate nohighlight">\(a(t,\tau)\)</span> which are used in the weighting sum of the <span class="math notranslate nohighlight">\(v(t)\)</span>:
<span class="math notranslate nohighlight">\(e(\tau) = \sum_t a(t,\tau) v(t)\)</span>.</p>
</section>
</section>
<section id="rnn-lstm">
<h2>RNN/ LSTM<a class="headerlink" href="#rnn-lstm" title="Link to this heading">#</a></h2>
</section>
<section id="transformer-self-attention">
<h2>Transformer/ Self-Attention<a class="headerlink" href="#transformer-self-attention" title="Link to this heading">#</a></h2>
<p>ViT</p>
<p>AST</p>
<p>HTS-AT</p>
</section>
<section id="conformer">
<h2>Conformer<a class="headerlink" href="#conformer" title="Link to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="bricks_bottleneck.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Bottleneck</p>
      </div>
    </a>
    <a class="right-next"
       href="bricks_paradigm.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Paradigms</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#u-net">U-Net</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#many-to-one-reducing-the-time-dimensions">Many to One: reducing the time dimensions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-weighting">Attention weighting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-pool">Auto-Pool</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-models">Using models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn-lstm">RNN/ LSTM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-self-attention">Transformer/ Self-Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conformer">Conformer</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Geoffroy Peeters, Gabriel Meseguer-Brocal, Alain Riou, Stefan Lattner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>