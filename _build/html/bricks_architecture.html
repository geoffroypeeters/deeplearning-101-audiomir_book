

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Architectures &#8212; Deep Learning 101 for Audio-based MIR</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'bricks_architecture';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Paradigms" href="bricks_paradigm.html" />
    <link rel="prev" title="Bottleneck" href="bricks_bottleneck.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="front.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/wave.png" class="logo__image only-light" alt="Deep Learning 101 for Audio-based MIR - Home"/>
    <script>document.write(`<img src="_static/wave.png" class="logo__image only-dark" alt="Deep Learning 101 for Audio-based MIR - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="front.html">
                    Deep Learning 101 for Audio-based MIR
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="intro.html">Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="intro_dataset.html">Datasets .hdf5/.pyjama</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro_pytorch.html">Pytorch dataset/dataloader</a></li>

<li class="toctree-l2"><a class="reference internal" href="intro_lightining.html">TorchLightning training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notebook.html">Nodebooks in Colab</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musiccontent.html">Music Content Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_autotagging_frontend.html">Auto-Tagging (front-ends)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_multipitchestimation.html">Multi-Pitch-Estimation (MPE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_coverdetection.html">Cover Song Identification (CSI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_autotagging_ssl.html">Auto-Tagging (SSL)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musicprocessing.html">Music Audio Processing</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_sourceseparation.html">Source Separation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musicgeneration.html">Musical Audio Generation</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_basics.html">Basics of Generative Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_early.html">Early Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_auto.html">Autoregressive Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_diff.html">Generation with Latent Diffusion</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Bricks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bricks_input.html">Inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_frontend.html">Front-ends</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_projection.html">Projections</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_bottleneck.html">Bottleneck</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_paradigm.html">Paradigms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="biography.html">About the authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibiography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fbricks_architecture.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/bricks_architecture.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Architectures</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#u-net">U-Net</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#many-to-one-reducing-the-time-dimensions">Many to One: reducing the time dimensions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-weighting">Attention weighting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-pool">Auto-Pool</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-models">Using models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-architectures">Recurrent Architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn">RNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm">LSTM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-self-attention">Transformer/Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-example">Self-Attention Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="architectures">
<h1>Architectures<a class="headerlink" href="#architectures" title="Permalink to this heading">#</a></h1>
<p><img alt="top" src="_images/top.png" /></p>
<p>We denote by <code class="docutils literal notranslate"><span class="pre">architecture</span></code> the overall design of a neural network, i.e. the way front-end and projections are used together.</p>
<section id="u-net">
<span id="lab-unet"></span><h2>U-Net<a class="headerlink" href="#u-net" title="Permalink to this heading">#</a></h2>
<p>The U-Net was proposed in <span id="id1">[<a class="reference internal" href="bibiography.html#id56" title="Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells III, and Alejandro F. Frangi, editors, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, volume 9351 of Lecture Notes in Computer Science, 234–241. Springer, 2015. URL: https://doi.org/10.1007/978-3-319-24574-4\_28, doi:10.1007/978-3-319-24574-4\_28.">RFB15</a>]</span> in the framework of biomedical image segmentation and made popular in MIR by <span id="id2">[<a class="reference internal" href="bibiography.html#id55" title="Andreas Jansson, Eric J. Humphrey, Nicola Montecchio, Rachel M. Bittner, Aparna Kumar, and Tillman Weyde. Singing voice separation with deep u-net convolutional networks. In Sally Jo Cunningham, Zhiyao Duan, Xiao Hu, and Douglas Turnbull, editors, Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, 745–751. 2017. URL: https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171\_Paper.pdf.">JHM+17</a>]</span> for singing voice separation.</p>
<p>The U-Net is an auto-encoder with skip-connections.</p>
<ul class="simple">
<li><p>The <strong>encoder</strong> (left part) downsample the spatial dimensions and increase the depth,</p></li>
<li><p>The <strong>decoder</strong> (right part) upsample the spatial dimensions and decrease the depth.</p></li>
</ul>
<p><strong>Skip connections</strong> are added between equivalent layers of the encoder and decoder:</p>
<ul class="simple">
<li><p>example: the 256 channels level of the encoder is concatenated with the 256 level of the decoder to form a 512 tensor.</p></li>
<li><p>The goal of the skip-connections are two-folds:</p>
<ul>
<li><p>to bring back details of the original images to the decoder (the bottleneck being to compressed to represent detailed information)</p></li>
<li><p>to facilitate the backpropagation of the gradient.</p></li>
</ul>
</li>
</ul>
<p>The <strong>upsampling</strong> (decoder) part can be done either</p>
<ul class="simple">
<li><p>using Transposed Convolution (hence a well-known checkerboard artefact may appears)</p></li>
<li><p>using Interpolation followed by Normal convolution</p></li>
</ul>
<p><img alt="brick_unet" src="_images/brick_unet.png" /><br />
<strong>Figure</strong> U-Net architecture for biomedical image segmentation <em>image source: <span id="id3">[<a class="reference internal" href="bibiography.html#id56" title="Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: convolutional networks for biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells III, and Alejandro F. Frangi, editors, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, volume 9351 of Lecture Notes in Computer Science, 234–241. Springer, 2015. URL: https://doi.org/10.1007/978-3-319-24574-4\_28, doi:10.1007/978-3-319-24574-4\_28.">RFB15</a>]</span></em></p>
</section>
<section id="many-to-one-reducing-the-time-dimensions">
<h2>Many to One: reducing the time dimensions<a class="headerlink" href="#many-to-one-reducing-the-time-dimensions" title="Permalink to this heading">#</a></h2>
<p>They are many different ways to reduce a (temporal) sequence of embeddings <span class="math notranslate nohighlight">\(\{ \mathbf{x}_1, \ldots \mathbf{x}_{T_x}\}\)</span> to a single embedding <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> (Many-to-One).</p>
<p>Such a mechanism can be necessary in order to map the temporal embedding provided by the last layer of a network to a single ground-truth (such as in auto-tagging, where the whole track is from a given genre, or in Acoustic Scene Classification).</p>
<p>The most simple way to achieve this is to use the Mean/Average value (Average Pooling) or Maximum value (Max Pooling) of the <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> over time <span class="math notranslate nohighlight">\(t\)</span> (as done for example in <span id="id4">[<a class="reference internal" href="bibiography.html#id61">Die14</a>]</span>).</p>
<section id="attention-weighting">
<span id="lab-attentionweighting"></span><h3>Attention weighting<a class="headerlink" href="#attention-weighting" title="Permalink to this heading">#</a></h3>
<p>Another possibility is to compute a weighted sum of the values <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> where the weights <span class="math notranslate nohighlight">\(a_t\)</span> are denoted by <strong>attention</strong> parameters:
<span class="math notranslate nohighlight">\(\mathbf{x} = \sum_{t=0}^{T_x-1} a_t \mathbf{x}_t\)</span></p>
<p>In <span id="id5">[<a class="reference internal" href="bibiography.html#id59" title="Siddharth Gururani, Mohit Sharma, and Alexander Lerch. An attention mechanism for musical instrument recognition. In Arthur Flexer, Geoffroy Peeters, Julián Urbano, and Anja Volk, editors, Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands, November 4-8, 2019, 83–90. 2019. URL: http://archives.ismir.net/ismir2019/paper/000007.pdf.">GSL19</a>]</span>, it is proposed to compute these weights <span class="math notranslate nohighlight">\(a_t\)</span> either</p>
<ol class="arabic simple">
<li><p>by computing a new projection of the <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> and then normalize them:
<span class="math notranslate nohighlight">\(a_t = \frac{\sigma(\mathbf{v}^T h(\mathbf{x}_t))}{\sum_{\tau} \sigma(\mathbf{v}^T h(\mathbf{x}_{\tau}))}\)</span></p>
<ul class="simple">
<li><p>with <span class="math notranslate nohighlight">\(h\)</span> a learnable embedding, <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> the learned parameters of the attention layer</p></li>
</ul>
</li>
<li><p>doing the same after splitting <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> in two (along the channel dimensions): the first part being used to compute “values”, the second to compute “weights”</p></li>
</ol>
<p><img alt="brick_attention_instrument" src="_images/brick_attention_instrument.png" /><br />
<strong>Figure</strong> Attention weighting, <em>image source: <span id="id6">[<a class="reference internal" href="bibiography.html#id59" title="Siddharth Gururani, Mohit Sharma, and Alexander Lerch. An attention mechanism for musical instrument recognition. In Arthur Flexer, Geoffroy Peeters, Julián Urbano, and Anja Volk, editors, Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands, November 4-8, 2019, 83–90. 2019. URL: http://archives.ismir.net/ismir2019/paper/000007.pdf.">GSL19</a>]</span></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">nnSoftmaxWeight</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform attention weighing based on softmax with channel splitting</span>
<span class="sd">    Code from https://github.com/furkanyesiler/move</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_channel</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_channel</span> <span class="o">=</span> <span class="n">nb_channel</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nb_channel</span><span class="o">/</span><span class="mi">2</span><span class="p">):],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nb_channel</span><span class="o">/</span><span class="mi">2</span><span class="p">)]</span> <span class="o">*</span> <span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</section>
<section id="auto-pool">
<span id="lab-autopoolweightsplit"></span><h3>Auto-Pool<a class="headerlink" href="#auto-pool" title="Permalink to this heading">#</a></h3>
<p>The above attention mechanism can by combined with the auto-pool operators proposed by <span id="id7">[<a class="reference internal" href="bibiography.html#id60" title="Brian McFee, Justin Salamon, and Juan Pablo Bello. Adaptive pooling operators for weakly labeled sound event detection. IEEE ACM Trans. Audio Speech Lang. Process., 26(11):2180–2193, 2018. URL: https://doi.org/10.1109/TASLP.2018.2858559, doi:10.1109/TASLP.2018.2858559.">MSB18</a>]</span>.</p>
<p>The auto-pool operators is defined as</p>
<div class="math notranslate nohighlight">
\[\tilde{\mathbf{x}}_t = \frac{\exp(\alpha \cdot \mathbf{x}_t)}{\sum_{\tau} \exp(\alpha \cdot \mathbf{x}_{\tau})}\]</div>
<p>It uses a parameter <span class="math notranslate nohighlight">\(\alpha\)</span> which allows to range from</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha=0\)</span> (unweighted, a.k.a. average pooling),</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha=1\)</span> (softmax weighted mean),</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha=\infty\)</span>: (a.k.a. max pooling).</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(\alpha\)</span> parameters is a trainable parameters (optimized using SGD).</p>
<p><img alt="brick_autopool" src="_images/brick_autopool.png" /><br />
<strong>Figure</strong> Auto-pool operator <em>image source: <span id="id8">[<a class="reference internal" href="bibiography.html#id60" title="Brian McFee, Justin Salamon, and Juan Pablo Bello. Adaptive pooling operators for weakly labeled sound event detection. IEEE ACM Trans. Audio Speech Lang. Process., 26(11):2180–2193, 2018. URL: https://doi.org/10.1109/TASLP.2018.2858559, doi:10.1109/TASLP.2018.2858559.">MSB18</a>]</span></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Code: https://github.com/furkanyesiler/move</span>
<span class="k">def</span> <span class="nf">f_autopool_weights</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">autopool_param</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculating the autopool weights for a given tensor</span>
<span class="sd">    :param data: tensor for calculating the softmax weights with autopool</span>
<span class="sd">    :return: softmax weights with autopool</span>

<span class="sd">    see https://arxiv.org/pdf/1804.10070</span>
<span class="sd">    alpha=0: unweighted mean</span>
<span class="sd">    alpha=1: softmax</span>
<span class="sd">    alpha=inf: max-pooling</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># --- x: (batch, 256, 1, T)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">data</span> <span class="o">*</span> <span class="n">autopool_param</span>
    <span class="c1"># --- max_values: (batch, 256, 1, 1)</span>
    <span class="n">max_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="c1"># --- softmax (batch, 256, 1, T)</span>
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">max_values</span><span class="p">)</span>
    <span class="c1"># --- weights (batch, 256, 1, T)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">softmax</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">softmax</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights</span>
</pre></div>
</div>
</section>
<section id="using-models">
<h3>Using models<a class="headerlink" href="#using-models" title="Permalink to this heading">#</a></h3>
<p>It is also possible to use a <strong>RNN/LSTM in Many-to-One configuration</strong> (only the last hidden state <span class="math notranslate nohighlight">\(\mathbf{x}_{T_x}\)</span> is mapped to an output <span class="math notranslate nohighlight">\(\hat{y}\)</span>).</p>
<p>Finally it is possible to add an extra CLASS token to a Transformer architecture.</p>
<p><img alt="brick_pooling" src="_images/brick_pooling_P.png" /></p>
<p>It should be noted that the term “Attention” encapsulates a large set of different paradigms.</p>
<ul class="simple">
<li><p>In the <strong>encode-decoder architecture</strong> <span id="id9">[<a class="reference internal" href="bibiography.html#id58" title="Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. 2015. URL: http://arxiv.org/abs/1409.0473.">BCB15</a>]</span> it is used during decoding to define the correct context <span class="math notranslate nohighlight">\(\mathbf{c}_{\tau}\)</span> to be used to generate the hidden state <span class="math notranslate nohighlight">\(\mathbf{s}_{\tau}\)</span>.
For this it compares the decoder hidden state <span class="math notranslate nohighlight">\(\mathbf{s}_{\tau-1}\)</span> to all the encoder hidden states <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span>.</p></li>
<li><p>In the <strong>transformer architecture</strong> <span id="id10">[<a class="reference internal" href="bibiography.html#id57" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 5998–6008. 2017. URL: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.">VSP+17</a>]</span> it is used to compute a self-attention.
For this, the <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> are mapped (using matrix projections) to query <span class="math notranslate nohighlight">\(\mathbf{q}_t\)</span>, key <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> and values <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span>.
A given <span class="math notranslate nohighlight">\(\mathbf{q}_{\tau}\)</span> is then compared to all <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> to compute attention weights <span class="math notranslate nohighlight">\(\mathbf{a}_{t,\tau}\)</span> which are used in the weighting sum of the <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span>:
<span class="math notranslate nohighlight">\(\mathbf{e}_{\tau} = \sum_t \mathbf{a}_{t,\tau} \mathbf{v}_{t}\)</span>.</p></li>
</ul>
</section>
</section>
<section id="recurrent-architectures">
<h2>Recurrent Architectures<a class="headerlink" href="#recurrent-architectures" title="Permalink to this heading">#</a></h2>
<section id="rnn">
<span id="lab-rnn"></span><h3>RNN<a class="headerlink" href="#rnn" title="Permalink to this heading">#</a></h3>
<p><strong>Recurrent Neural Networks (RNNs)</strong> are a type of neural network designed to work with <mark>sequential data</mark> (e.g., time series, text, etc.).
They “remember” information from previous inputs by using hidden states, which allows them to model dependencies across time steps.</p>
<p>Their generic formulation for inputs <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> over time is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{h}_t = \tanh (\mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{W}_{hx} \mathbf{x}_t+ \mathbf{b}_h)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span> is the hidden state of the RNN at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>A <mark>bi-directional-RNN</mark>, read the data in both directions (left-to-right and right-to-left).
The goal is to make <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span> both dependent on <span class="math notranslate nohighlight">\(\mathbf{h}_{t-1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}_{t+1}\)</span>.</p>
<p>Two configurations are often used with RNNs:</p>
<ul class="simple">
<li><p><mark>Many-to-Many</mark>: RNN can be used to model the evaluation over time of features (such as done in the past with Kalman filters or HMM).
They are often used to represent a Language model.</p></li>
<li><p><mark>Many-to-One</mark>: One can also use the last hidden state of a RNN <span class="math notranslate nohighlight">\(\mathbf{h}_{T_x}\)</span> where <span class="math notranslate nohighlight">\(T_x\)</span> is the length of the input sequence, to sum up the content of the input sequence (see picture below).</p></li>
</ul>
<p><img alt="brick_rnn" src="_images/brick_rnn.png" /><br />
<strong>Figure</strong> RNN in Many-to-Many and Many-to-One configurations <em>image source: <a class="reference external" href="https://www.researchgate.net/figure/The-four-types-of-recurrent-neural-network-architectures-a-univariate-many-to-one_fig3_317192370">Link</a></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="lstm">
<h3>LSTM<a class="headerlink" href="#lstm" title="Permalink to this heading">#</a></h3>
<p><strong>Long Short-Term Memory (LSTM)</strong>  are a specialized type of RNN designed to handle long-term dependencies more effectively.
LSTM use a more complex architecture with</p>
<ul class="simple">
<li><p>a memory <span class="math notranslate nohighlight">\(\mathbf{c}_t\)</span> over time <span class="math notranslate nohighlight">\(t\)</span>,</p></li>
<li><p>a hidden value <span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span> and</p></li>
<li><p>a set of gates (input gate, forget gate, and output gate)</p>
<ul>
<li><p>they allow to control the flow of information between the input <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span>, the previous hidden state <span class="math notranslate nohighlight">\(\mathbf{h}_{t-1}\)</span> and memory <span class="math notranslate nohighlight">\(\mathbf{c}_{t-1}\)</span> and their new values.</p></li>
</ul>
</li>
</ul>
<p>This allows them to retain relevant information over longer sequences while “forgetting” irrelevant information.</p>
<p><strong>A critical reason why LSTMs work better than RNNs</strong> is that the memory cell provides a path for information to flow across time steps without repeatedly passing it through non-linearities (e.g., <code class="docutils literal notranslate"><span class="pre">torch.nn.Sigmoid</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.nn.Tanh</span></code>).
This principle mitigates the vanishing gradient problem and is similar to ResNets and the residual stream in Transformers, where skip connections allow information to bypass layers that add non-linearities.</p>
<p>As in RNNs, two configurations are often used with LSTMs:</p>
<ul class="simple">
<li><p><mark>Many-to-many</mark></p></li>
<li><p><mark>Many-to-one</mark></p></li>
</ul>
<p><img alt="brick_rnn" src="_images/brick_lstm.png" /><br />
<strong>Figure</strong> Details of a LSTM cell  <em>image source: <a class="reference external" href="https://mlarchive.com/deep-learning/understanding-long-short-term-memory-networks/">Link</a></em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="transformer-self-attention">
<h2>Transformer/Self-Attention<a class="headerlink" href="#transformer-self-attention" title="Permalink to this heading">#</a></h2>
<figure class="align-default" id="brick-transformer">
<span id="lab-transformer-fig"></span><a class="reference internal image-reference" href="_images/brick_transformer.png"><img alt="_images/brick_transformer.png" src="_images/brick_transformer.png" style="width: 70%;" /></a>
</figure>
<p><strong>Figure:</strong> The Transformer - model architecture.</p>
<p>In recent years, Transformers <span id="id11">[<a class="reference internal" href="bibiography.html#id57" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 5998–6008. 2017. URL: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.">VSP+17</a>]</span> widely replaced recurrent architectures for sequence modeling tasks and are also increasingly used instead of convolutional architectures.
Their signature component, the <strong>attention mechanism</strong>, gives them a unique advantage over previous architectures.
There are several intuitive explanations for the attention mechanism (e.g., attending to important tokens, address-based memory access).
Independent of how one thinks about the attention mechanism, its result is an attention matrix (resembling a weight matrix) that is input-dependent, while most other architectures employ weight matrices whose parameters are fixed at inference time.</p>
<p>For sequence modeling (cf. <a class="reference internal" href="task_musicgeneration_auto.html#lab-architecture-auto"><span class="std std-ref">our autoregressive generation example</span></a>), we usually employ a <strong>causal transformer</strong> where attention matrices are masked so that future information cannot be taken into consideration (indicated by <code class="docutils literal notranslate"><span class="pre">Masked</span> <span class="pre">Multi-Head</span> <span class="pre">Attention</span></code> in the figure above).
For that, we only use the <strong>DECODER</strong> part, while for non-causal tasks like <em>masked token prediction</em> <span id="id12">[<a class="reference internal" href="bibiography.html#id5" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), 4171–4186. Association for Computational Linguistics, 2019.">DCLT19</a>]</span>, the <strong>ENCODER</strong> part is used.
Using both, an encoder with cross-connections to the decoder, as proposed in the initial paper, is mainly used to inject conditioning information if needed.</p>
<p>Note that after every <code class="docutils literal notranslate"><span class="pre">Multi-Head</span> <span class="pre">Attention</span></code> or <code class="docutils literal notranslate"><span class="pre">Feed</span> <span class="pre">Forward</span></code> module, there is an <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">&amp;</span> <span class="pre">Norm</span></code> operation.
This means, the input to each module is added to its output, resulting in a “residual stream”, where information is written into or retrieved from.
From a simplified point of view, it is now understood that the <code class="docutils literal notranslate"><span class="pre">Mult-Head</span> <span class="pre">Attention</span></code> modules rather combine and shuffle information from the residual stream, while weights of the <code class="docutils literal notranslate"><span class="pre">Feed</span> <span class="pre">Forward</span></code> modules act as “memories” that inject new information into the residual stream.</p>
<section id="self-attention-example">
<h3>Self-Attention Example<a class="headerlink" href="#self-attention-example" title="Permalink to this heading">#</a></h3>
<p>This section gives an explanation of self-attention that is <span style="color: red;">illustrative but very simplified</span>.
In practice, tokens are not full words but rather word fragments.
Keys, values and queries are continuous vectors whose meaning is not as simple and discrete as in the example below, and a token can attend to more than one value.
However, the example is correct in how information is propagated through a self-attention layer and could theoretically happen as described.</p>
<p><img alt="brick_attention" src="_images/brick_attention.png" />
<strong>Figure:</strong> Simple self-attention example.</p>
<p>In self-attention, every <em>token</em> (every <em>word</em> in the example above), is represented by an embedding vector.
By multiplying every such token embedding with three fixed matrices (<span class="math notranslate nohighlight">\(W^K\)</span>, <span class="math notranslate nohighlight">\(W^V\)</span> and <span class="math notranslate nohighlight">\(W^Q\)</span>) we obtain a key, value and query vector for every position.</p>
<p>In our simplified example, the model may have learned to emit a <em>key</em> vector that stands for <mark>verb</mark> from a token embedding that stands for <mark>chasing</mark>, effectively saying “i am a verb!”.
For the <mark>dog</mark> embedding, it may ask “what is the dog doing?” and therefore emitting a query resembling the <mark>verb</mark> key.
The result of the self-attention is then to copy the value to whereever a query fits the respective key:</p>
<figure class="align-default" id="brick-attention2">
<a class="reference internal image-reference" href="_images/brick_attention2.png"><img alt="_images/brick_attention2.png" src="_images/brick_attention2.png" style="width: 83%;" /></a>
</figure>
<p><strong>Figure:</strong> Result of the simple self-attention example.</p>
<p>As indicated in the <a class="reference internal" href="#lab-transformer-fig"><span class="std std-ref">architecture diagram</span></a>, after every attention layer, there is an <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">&amp;</span> <span class="pre">Norm</span></code> operation.
In our example, we start from the <mark>dog</mark> embedding (i.e., the <mark>dog</mark> position in a semantic space), and add the <mark>chase</mark> vector, effectively augmenting <mark>dog</mark> by moving into the <mark>chase</mark> direction.
As a result, we obtain a “chasing dog” that can then be further transformed in subsequent layers.
Through iterative, relative transformations of such embeddings in a semantic space, we can thereby resolve complex relationships and perform precise, final predictions.</p>
</section>
<section id="positional-encoding">
<h3>Positional Encoding<a class="headerlink" href="#positional-encoding" title="Permalink to this heading">#</a></h3>
<p>Note that in the example above, the results would occur the same way if the order of the input sequence would be shuffled (i.e., the <mark>chase</mark> vector would also be added to the <mark>dog</mark> position).
…</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="bricks_bottleneck.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Bottleneck</p>
      </div>
    </a>
    <a class="right-next"
       href="bricks_paradigm.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Paradigms</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#u-net">U-Net</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#many-to-one-reducing-the-time-dimensions">Many to One: reducing the time dimensions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-weighting">Attention weighting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-pool">Auto-Pool</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-models">Using models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent-architectures">Recurrent Architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn">RNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm">LSTM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-self-attention">Transformer/Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-example">Self-Attention Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Geoffroy Peeters, Gabriel Meseguer-Brocal, Alain Riou, Stefan Lattner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>