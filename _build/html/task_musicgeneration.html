

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Musical Audio Generation &#8212; Deep Learning 101 for Audio-based MIR</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'task_musicgeneration';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Basics of Generative Modeling" href="task_musicgeneration_basics.html" />
    <link rel="prev" title="Source Separation" href="task_sourceseparation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="front.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/wave.png" class="logo__image only-light" alt="Deep Learning 101 for Audio-based MIR - Home"/>
    <script>document.write(`<img src="_static/wave.png" class="logo__image only-dark" alt="Deep Learning 101 for Audio-based MIR - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="front.html">
                    Deep Learning 101 for Audio-based MIR
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Abstract</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="abstract.html">Abstract</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="intro.html">Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="intro_dataset.html">Datasets .hdf5/.pyjama</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro_pytorch.html">Pytorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro_lightining.html">TorchLightning training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notebook.html">Notebooks in Colab</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musiccontent.html">Music Audio Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_multipitchestimation.html">Multi-Pitch-Estimation (MPE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_coverdetection.html">Cover Song Identification (CSI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_autotagging_frontend.html">Auto-Tagging (front-ends)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_autotagging_ssl.html">Auto-Tagging (SSL)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musicprocessing.html">Music Audio Processing</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_sourceseparation.html">Source Separation</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Musical Audio Generation</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_basics.html">Basics of Generative Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_early.html">Early Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_auto.html">Autoregressive Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_diff.html">Generation with Latent Diffusion</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Bricks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bricks_input.html">Inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_frontend.html">Front-ends</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_projection.html">Projections</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_bottleneck.html">Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_architecture.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_paradigm.html">Paradigms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="biography.html">About the authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibiography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Ftask_musicgeneration.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/task_musicgeneration.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Musical Audio Generation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-contents">Table of Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-the-task">Goal of the Task</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-datasets">Popular Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-the-task-evaluated">How is the Task Evaluated?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frechet-audio-distance-fad">Frechet Audio Distance (FAD)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#origins-and-motivation">Origins and Motivation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calculation-of-fad">Calculation of FAD</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-fad">Applications of FAD</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-methods">Inception Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-score-is">Inception Score (IS)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-inception-distance-kid">Kernel Inception Distance (KID)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subjective-evaluation">Subjective Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#without-reference-samples">Without Reference Samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#with-reference-samples">With Reference Samples</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="musical-audio-generation">
<h1>Musical Audio Generation<a class="headerlink" href="#musical-audio-generation" title="Permalink to this heading">#</a></h1>
<section id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Basics of Generative Modeling</p>
<ul>
<li><p><a class="reference internal" href="task_musicgeneration_basics.html#lab-autoregressive"><span class="std std-ref">Autoregressive Generation</span></a></p></li>
<li><p><a class="reference internal" href="task_musicgeneration_basics.html#lab-parallel"><span class="std std-ref">Non-Autoregressive (Parallel) Generation</span></a></p></li>
</ul>
</li>
<li><p>Early Works</p>
<ul>
<li><p><a class="reference internal" href="task_musicgeneration_early.html#lab-wavenet"><span class="std std-ref">WaveNet</span></a></p></li>
<li><p><a class="reference internal" href="task_musicgeneration_early.html#lab-samplernn"><span class="std std-ref">SampleRNN</span></a></p></li>
<li><p><a class="reference internal" href="task_musicgeneration_early.html#lab-gans2"><span class="std std-ref">Generative Adversarial Networks (GANs)</span></a></p></li>
</ul>
</li>
<li><p>Examples</p>
<ul>
<li><p><a class="reference internal" href="task_musicgeneration_auto.html#lab-ex-autoregressive"><span class="std std-ref">Autoregressive Generation:</span></a> <a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Generation_Autoregressive.ipynb">Notebook</a></p></li>
<li><p><a class="reference internal" href="task_musicgeneration_diff.html#lab-ex-diffusion"><span class="std std-ref">Generation with Latent Diffusion:</span></a> <a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Generation_Diffusion.ipynb">Notebook</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="goal-of-the-task">
<h2>Goal of the Task<a class="headerlink" href="#goal-of-the-task" title="Permalink to this heading">#</a></h2>
<p>Musical audio generation aims to create various musical content, from <mark>individual notes</mark> <span id="id1">[<a class="reference internal" href="bibiography.html#id61" title="Jesse H. Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam Roberts. Gansynth: adversarial neural audio synthesis. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL: https://openreview.net/forum?id=H1xQVn09FX.">EAC+19</a>]</span> to <mark>instrumental accompaniments/arrangements</mark> <span id="id2">[<a class="reference internal" href="bibiography.html#id55" title="Javier Nistal, Marco Pasini, Cyran Aouameur, Maarten Grachten, and Stefan Lattner. Diff-a-riff: musical accompaniment co-creation via latent diffusion models. CoRR, 2024.">NPA+24</a>]</span> and <mark>complete songs</mark> <span id="id3">[<a class="reference internal" href="bibiography.html#id11" title="Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. In ICML. OpenReview.net, 2024.">ECT+24</a>]</span>.
In the <mark>early days</mark> of audio generation research, methods often focused on producing audio directly in the <mark>time or time-frequency domain</mark>.
Recent approaches, however, work with <mark>compressed representations</mark>, often using <mark>neural audio codecs</mark>.</p>
<p>The most widely used models today are <mark>autoregressive</mark> (Transformer) architectures and <mark>diffusion</mark> models. Autoregressive architectures are particularly effective for discrete codecs, while diffusion models are better suited for continuous representations.</p>
</section>
<section id="popular-datasets">
<h2>Popular Datasets<a class="headerlink" href="#popular-datasets" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>NSynth</strong>: NSynth is a dataset designed for musical audio generation, often regarded as the “MNIST” of audio.
It contains 305,979 short, single-note audio samples from 1,006 instruments, each labeled with attributes such as pitch, velocity, and instrument family.
This dataset is valuable for early experiments in audio synthesis and machine learning. <a class="reference external" href="https://magenta.tensorflow.org/datasets/nsynth">NSynth Dataset</a></p></li>
<li><p><strong>MusicNet</strong>: MusicNet is a collection of 330 freely-licensed classical music recordings, totaling over 34 hours of audio.
It includes more than 1 million annotated labels indicating the precise timing, instrument, and position of each note in the compositions.
This dataset is suitable for tasks involving complex musical structures and note prediction. <a class="reference external" href="https://www.kaggle.com/datasets/imsparsh/musicnet-dataset">MusicNet Dataset</a></p></li>
<li><p><strong>MAESTRO</strong>: The MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) dataset features over 200 hours of virtuosic piano performances captured with aligned MIDI and audio recordings.
It includes detailed metadata such as composer, title, and year of performance, making it particularly useful for training models in high-quality piano music generation. <a class="reference external" href="https://magenta.tensorflow.org/datasets/maestro">MAESTRO Dataset</a></p></li>
<li><p><strong>MagnaTagATune</strong>: MagnaTagATune offers a large collection of music annotated with tags, useful for genre classification and multi-label tasks.
It contains over 25,000 music clips, each 29 seconds long, with multiple annotations per clip, covering a wide range of genres and instruments. <a class="reference external" href="http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset">MagnaTagATune Dataset</a></p></li>
</ul>
</section>
<section id="how-is-the-task-evaluated">
<h2>How is the Task Evaluated?<a class="headerlink" href="#how-is-the-task-evaluated" title="Permalink to this heading">#</a></h2>
<p>Evaluation of generation tasks is difficult. In other ML tasks, specific targets (e.g., labels, data points) are available in a given evaluation set, allowing precision estimation for a given model. In contrast, in <mark>audio generation</mark>, the goal is to <mark>sample from the distribution of the training set without directly reproducing any training data</mark>.</p>
<p>As a result, indirect, <mark>distribution-based evaluation metrics</mark> are commonly used rather than relying on one-to-one comparisons, as in autoencoders or classification tasks.</p>
<section id="frechet-audio-distance-fad">
<h3>Frechet Audio Distance (FAD)<a class="headerlink" href="#frechet-audio-distance-fad" title="Permalink to this heading">#</a></h3>
<p>Nowadays, the most commonly used metric in assessing the quality of generated audio is the Frechet Audio Distance (FAD) <span id="id4">[<a class="reference internal" href="bibiography.html#id17" title="Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms. In INTERSPEECH, 2350–2354. ISCA, 2019.">KZRS19</a>]</span>.
It <mark>compares the statistics</mark> of generated audio to those of real, high-quality reference samples (usually the <mark><em>test set</em></mark>) in the embedding space of a pre-trained model.
The idea is to assess the “closeness” of the two distributions: one for the generated samples and one for real samples.</p>
<section id="origins-and-motivation">
<h4>Origins and Motivation<a class="headerlink" href="#origins-and-motivation" title="Permalink to this heading">#</a></h4>
<figure class="align-default" id="fad-illustration">
<a class="reference internal image-reference" href="_images/brick_fad.png"><img alt="_images/brick_fad.png" src="_images/brick_fad.png" style="width: 80%;" /></a>
</figure>
<p><strong>Figure 1:</strong> FAD computation overview for a <mark>music enhancement system</mark> as initially proposed (image source: <span id="id5">[<a class="reference internal" href="bibiography.html#id17" title="Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms. In INTERSPEECH, 2350–2354. ISCA, 2019.">KZRS19</a>]</span>).</p>
<p>Fréchet Audio Distance was initially developed to evaluate music enhancement algorithms (see Figure 1).
It filled a gap in objective audio quality evaluation, especially in generative tasks like music synthesis, audio inpainting, and speech generation.
Before FAD, audio evaluations often relied on metrics like mean squared error (for reconstruction-based approaches) or subjective listening tests.
While subjective tests remain essential for evaluating the perceptual quality of audio, FAD offers a more automated, quantifiable approach that aligns with perceptual quality.</p>
</section>
<section id="calculation-of-fad">
<h4>Calculation of FAD<a class="headerlink" href="#calculation-of-fad" title="Permalink to this heading">#</a></h4>
<figure class="align-default" id="fad-distribution">
<a class="reference internal image-reference" href="_images/overlap_fad.png"><img alt="_images/overlap_fad.png" src="_images/overlap_fad.png" style="width: 60%;" /></a>
</figure>
<p><strong>Figure 2:</strong> Example of audio samples projected in a 2d embedding space. The FAD in this example is <mark><span class="math notranslate nohighlight">\(1.47\)</span></mark>.</p>
<p>The FAD metric works by embedding audio signals into a perceptual feature space using a <mark>pre-trained deep neural network</mark> model.
Initially a <mark>VGGish</mark> model was proposed, but it has been shown that <mark>LAION CLAP</mark> embeddings <span id="id6">[<a class="reference internal" href="bibiography.html#id14" title="Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP, 1–5. IEEE, 2023.">WCZ+23</a>]</span> or a specific <mark>PANN</mark> model <span id="id7">[<a class="reference internal" href="bibiography.html#id13" title="Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D. Plumbley. Panns: large-scale pretrained audio neural networks for audio pattern recognition. IEEE ACM Trans. Audio Speech Lang. Process., 28:2880–2894, 2020.">KCI+20</a>]</span> align better with perceived audio quality <span id="id8">[<a class="reference internal" href="bibiography.html#id15" title="Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouilidou. Adapting frechet audio distance for generative music evaluation. In ICASSP, 1331–1335. IEEE, 2024.">GGBE24</a>, <a class="reference internal" href="bibiography.html#id16" title="Modan Tailleur, Junwon Lee, Mathieu Lagrange, Keunwoo Choi, Laurie M. Heller, Keisuke Imoto, and Yuki Okamoto. Correlation of fréchet audio distance with human perception of environmental audio is embedding dependant. CoRR, 2024.">TLL+24</a>]</span>.
Once embedded, it treats these embeddings as multidimensional distributions and calculates the <mark>Fréchet Distance</mark> (also known as the Wasserstein-2 distance) between the two distributions.</p>
<p>Mathematically, it involves <mark>comparing the means and covariances</mark> of these distributions:</p>
<div class="math notranslate nohighlight">
\[
\text{FAD} = \| \mu_r - \mu_g \|^2 + \text{Tr}( \Sigma_r + \Sigma_g - 2 (\Sigma_r \Sigma_g)^{1/2} )
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_r\)</span> and <span class="math notranslate nohighlight">\(\Sigma_r\)</span> are the mean and covariance of the reference (real) distribution, and <span class="math notranslate nohighlight">\(\mu_g\)</span> and <span class="math notranslate nohighlight">\(\Sigma_g\)</span> are those of the generated distribution.</p>
</section>
<section id="applications-of-fad">
<h4>Applications of FAD<a class="headerlink" href="#applications-of-fad" title="Permalink to this heading">#</a></h4>
<p>FAD is widely used in research on generative audio models, including:</p>
<ol class="arabic simple">
<li><p><strong>Music and Sound Generation</strong>: Evaluating GANs or VAEs that generate music, sound effects, or synthetic soundscapes.</p></li>
<li><p><strong>Speech Synthesis</strong>: Benchmarking TTS (Text-to-Speech) systems to gauge how close generated speech is to natural human voices.</p></li>
<li><p><strong>Audio Super-Resolution</strong>: Comparing high-resolution generated audio with real high-resolution samples.</p></li>
<li><p><strong>Denoising and Enhancement</strong>: Assessing the quality of denoised or enhanced audio by comparing it to clean reference audio.</p></li>
</ol>
<p>Since its introduction, FAD has become a standard metric for evaluating the realism and quality of generated or processed audio.</p>
</section>
</section>
<section id="inception-methods">
<h3>Inception Methods<a class="headerlink" href="#inception-methods" title="Permalink to this heading">#</a></h3>
<p>The following methods apply if class labels are available for the training data of a generative model.
They rely on a so-called <em>Inception network</em> – a classifier trained to predict the class labels from the data.</p>
<section id="inception-score-is">
<h4>Inception Score (IS)<a class="headerlink" href="#inception-score-is" title="Permalink to this heading">#</a></h4>
<p>The <strong>Inception Score</strong> is a metric that evaluates the quality and diversity of generated samples by measuring two key properties:
(1) <strong>high confidence</strong> in classifications (suggesting realistic, distinct samples) and
(2) <strong>diversity</strong> across classes (indicating a wide variety of generated outputs).</p>
<p>It uses a pre-trained Inception network to classify generated samples and calculates the score as follows:</p>
<div class="math notranslate nohighlight">
\[
\text{IS} = \exp \left( \mathbb{E}_x \left[ D_{\text{KL}}( p(y|x) \| p(y) ) \right] \right)
\]</div>
<p>In this formula:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( p(y|x) \)</span> represents the conditional class distribution given a generated sample <span class="math notranslate nohighlight">\( x \)</span>, where higher confidence corresponds to more realistic samples.</p></li>
<li><p><span class="math notranslate nohighlight">\( p(y) \)</span> is the marginal class distribution across samples, promoting diversity if samples cover a wide range of classes.</p></li>
<li><p>The <strong>Kullback-Leibler divergence</strong> <span class="math notranslate nohighlight">\( D_{\text{KL}} \)</span> between <span class="math notranslate nohighlight">\( p(y|x) \)</span> and <span class="math notranslate nohighlight">\( p(y) \)</span> is averaged over all generated samples, capturing the balance between realism and diversity. The exponential of this average yields the final Inception Score, with higher values indicating better quality.</p></li>
</ul>
<p>IS is commonly applied in image generation but can be adapted for use in generative audio and other domains.</p>
</section>
<section id="kernel-inception-distance-kid">
<h4>Kernel Inception Distance (KID)<a class="headerlink" href="#kernel-inception-distance-kid" title="Permalink to this heading">#</a></h4>
<p><strong>Kernel Inception Distance (KID)</strong> is a metric that evaluates the similarity between real and generated samples. Unlike IS, which only uses generated samples, KID compares the distributions of real and generated data using features extracted by a pre-trained Inception network. KID is calculated by computing the <strong>squared Maximum Mean Discrepancy (MMD)</strong> between the embeddings of real and generated samples, with a polynomial kernel for smoothing:</p>
<div class="math notranslate nohighlight">
\[
\text{KID} = \| \mathbb{E}[\phi(x_r)] - \mathbb{E}[\phi(x_g)] \|^2
\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\phi(x_r)\)</span> and <span class="math notranslate nohighlight">\(\phi(x_g)\)</span> represent the feature embeddings of real and generated samples, respectively.</p></li>
<li><p>MMD measures the difference in means of these embeddings, with KID summing the squared differences to capture distributional similarity.</p></li>
</ul>
<p>Unlike FAD, which uses covariance matrices, KID operates without assumptions about distribution shape and generally provides unbiased estimates, especially in small sample sizes. Lower KID values suggest that generated samples closely match the distribution of real samples, making it suitable for generative tasks requiring high-quality and realistic outputs.</p>
</section>
</section>
<section id="subjective-evaluation">
<h3>Subjective Evaluation<a class="headerlink" href="#subjective-evaluation" title="Permalink to this heading">#</a></h3>
<p>Objective evaluation metrics cannot capture all the details people care about when listening to audio.
Therefore, it is very common (and important) in audio generation works to perform user studies where human listeners assess the perceived quality of audio, focusing on aspects like naturalness, clarity, and overall fidelity.
For completeness, we also include methods in this section that are used to compare the audio quality of two or more audio files, typically used in audio enhancement, super resolution or restoration.</p>
<p>For reliable results in all methods, it is crucial to conduct tests in controlled listening environments, ideally with high-quality audio equipment.
<mark>Statistical analysis</mark> (like t-test) is often applied afterward to ensure the results are significant and unbiased.</p>
<section id="without-reference-samples">
<h4>Without Reference Samples<a class="headerlink" href="#without-reference-samples" title="Permalink to this heading">#</a></h4>
<p>The following metrics are used in cases where an absolute reference isn’t available, which typically applies to musical audio generation.</p>
<p><strong>Single Stimulus Rating (SSR)</strong> allows listeners to rate each sample individually.
This method is helpful when comparing samples of widely differing qualities, without needing a reference sample.
One of the most widely used SSR methods is the <strong>Mean Opinion Score (MOS)</strong>, where listeners rate each audio sample on a numerical scale, <mark>typically from 1 to 5</mark> (i.e., Likert Scale), with higher scores indicating better quality.
MOS is popular because it gives a straightforward average score for quality, often applied in areas like audio generation, speech synthesis and audio enhancement.</p>
<p>In order to obtain <mark>meaningful results</mark> for SSR in audio generation, samples from <mark>different datasets</mark> are presented to the user, for example <mark><em>real data</em></mark> and <mark><em>generated data</em></mark>.</p>
<p><strong>Attribute-Specific Rating (ASR)</strong> asks listeners to rate audio on <mark>specific qualities</mark>, like brightness, clarity, or naturalness, giving a more nuanced evaluation across multiple dimensions.
This approach is particularly useful when certain attributes are especially important, like naturalness in speech synthesis.</p>
</section>
<section id="with-reference-samples">
<h4>With Reference Samples<a class="headerlink" href="#with-reference-samples" title="Permalink to this heading">#</a></h4>
<p>In <strong>Comparison Category Rating (CCR)</strong>, listeners are presented with two audio samples, often a high-quality reference and a processed version, and rate the difference in quality between them.
This method helps detect subtle quality changes by making a direct comparison, which is useful for assessing codecs and noise reduction techniques.</p>
<p>For more precise distinctions, <strong>ABX Testing</strong> is a go-to approach.
In this test, listeners hear three samples—two known (A and B) and one unknown (X) that matches either A or B.
They must identify which one X corresponds to, revealing subtle perceptual differences.
ABX tests are commonly used to test the transparency of audio processing techniques.</p>
<p>Another evaluation approach is the <strong>Degradation Category Rating (DCR)</strong>, which involves rating the perceived degradation of an audio sample relative to a high-quality reference.
Listeners rate how much the quality has deteriorated, ranging from “imperceptible” to “very annoying,” making DCR effective in testing the negative impacts of audio processing and compression methods.</p>
<p>In the <strong>MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor)</strong> test, listeners rate multiple versions of the same audio on a scale from 0 to 100, with both high-quality and low-quality references included.
MUSHRA provides detailed insight across various conditions, making it useful for codec testing and audio enhancement evaluations.</p>
</section>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="task_sourceseparation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Source Separation</p>
      </div>
    </a>
    <a class="right-next"
       href="task_musicgeneration_basics.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Basics of Generative Modeling</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-contents">Table of Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-the-task">Goal of the Task</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-datasets">Popular Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-the-task-evaluated">How is the Task Evaluated?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frechet-audio-distance-fad">Frechet Audio Distance (FAD)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#origins-and-motivation">Origins and Motivation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calculation-of-fad">Calculation of FAD</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-fad">Applications of FAD</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-methods">Inception Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-score-is">Inception Score (IS)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-inception-distance-kid">Kernel Inception Distance (KID)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subjective-evaluation">Subjective Evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#without-reference-samples">Without Reference Samples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#with-reference-samples">With Reference Samples</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Geoffroy Peeters, Gabriel Meseguer-Brocal, Alain Riou, Stefan Lattner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>