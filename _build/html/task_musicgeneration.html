

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Musical Audio Generation &#8212; Deep Learning 101 for Audio-based MIR</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'task_musicgeneration';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inputs" href="bricks_input.html" />
    <link rel="prev" title="Auto-Tagging-SSL" href="task_autotagging_ssl.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="front.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/wave.png" class="logo__image only-light" alt="Deep Learning 101 for Audio-based MIR - Home"/>
    <script>document.write(`<img src="_static/wave.png" class="logo__image only-dark" alt="Deep Learning 101 for Audio-based MIR - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="front.html">
                    Deep Learning 101 for Audio-based MIR
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="task_autotagging_frontend.html">Auto-Tagging-FrontEnd</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_multipitchestimation.html">Multi-Pitch-Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_coverdetection.html">Cover Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_sourceseparation.html">Source Separation</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_autotagging_ssl.html">Auto-Tagging-SSL</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Musical Audio Generation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Bricks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bricks_input.html">Inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_frontend.html">Front-ends</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_projection.html">Projections</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_bottleneck.html">Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_architecture.html">Architectures</a></li>

<li class="toctree-l1"><a class="reference internal" href="bricks_paradigm.html">Paradigms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="biography.html">About the authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibiography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Ftask_musicgeneration.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/task_musicgeneration.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Musical Audio Generation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-the-task">Goal of the Task</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-datasets">Popular Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-musical-audio-synthesis">Neural Musical Audio Synthesis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-generative-modeling">Basics of Generative Modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-sequence-generation">Discrete Sequence Generation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-valued-data-generation">Continuous-Valued Data Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-autoencoders-vaes">Variational Autoencoders (VAEs)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-models">Diffusion Models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-works">Early Works</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#wavenet">WaveNet</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#samplernn">SampleRNN</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-adversarial-networks">Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-transformer-architectures">Autoregressive (Transformer) Architectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Diffusion Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-the-task-evaluated">How is the Task Evaluated?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frechet-audio-distance-fad">Frechet Audio Distance (FAD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subjective-evaluation">Subjective Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likert-scale">Likert Scale</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="musical-audio-generation">
<h1>Musical Audio Generation<a class="headerlink" href="#musical-audio-generation" title="Permalink to this heading">#</a></h1>
<section id="goal-of-the-task">
<h2>Goal of the Task<a class="headerlink" href="#goal-of-the-task" title="Permalink to this heading">#</a></h2>
<p>Musical audio generation aims to create various musical content, from individual notes to full instrumental arrangements and complete songs. In the early days of audio generation research, methods often focused on producing audio directly in the time or time-frequency domain. Recent approaches, however, work with compressed representations, often using neural audio codecs.</p>
<p>The most widely used models today are autoregressive (Transformer) architectures and diffusion models. Autoregressive architectures are particularly effective for discrete codecs, while diffusion models are better suited for continuous representations.</p>
</section>
<section id="popular-datasets">
<h2>Popular Datasets<a class="headerlink" href="#popular-datasets" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>NSynth</strong>: NSynth was once the go-to dataset for musical audio generation and can be regarded the “MNIST” in audio. It contains short, synthetic, single-note samples from different instrument families and detailed metadata, making it a valuable resource for early experiments.</p></li>
<li><p><strong>GTZAN (delete?)</strong>: The GTZAN dataset is often used for genre classification and can serve as a starting point for more complex audio generation tasks involving diverse genres.</p></li>
<li><p><strong>MusicNet</strong>: Contains recordings of classical music with aligned annotations, suitable for tasks involving complex musical structures.</p></li>
<li><p><strong>MAESTRO</strong>: The MAESTRO dataset features piano performances, providing MIDI and corresponding audio recordings. This makes it particularly useful for training models of high-quality piano music generation.</p></li>
<li><p><strong>MagnaTagATune</strong>: Offers a large collection of music with tags, useful for genre classification and multi-label tasks.</p></li>
</ul>
</section>
<section id="neural-musical-audio-synthesis">
<h2>Neural Musical Audio Synthesis<a class="headerlink" href="#neural-musical-audio-synthesis" title="Permalink to this heading">#</a></h2>
<section id="basics-of-generative-modeling">
<h3>Basics of Generative Modeling<a class="headerlink" href="#basics-of-generative-modeling" title="Permalink to this heading">#</a></h3>
<p>In generative tasks, it is necessary to inject <em>some form of stochasticity</em> into the generation process. In this regard, two general approaches can be distinguished: generation of <strong>discrete sequences</strong> and generation of <strong>continuous-valued data</strong>.
In this section, we will have a brief look into the two paradigms and give some examples of how they are modeled.</p>
<section id="discrete-sequence-generation">
<h4>Discrete Sequence Generation<a class="headerlink" href="#discrete-sequence-generation" title="Permalink to this heading">#</a></h4>
<p>For <strong>discrete sequences</strong>, models such as Recurrent Neural Networks (RNNs), Causal Convolutional Networks, and Transformers are typically trained with cross-entropy loss to output a probability distribution over discrete random variables in a deterministic manner. The stochasticity is then “injected” by sampling from that distribution.</p>
<p>At each time step <span class="math notranslate nohighlight">\(t\)</span>, the model outputs a probability distribution <span class="math notranslate nohighlight">\(P(y_t \mid y_{&lt;t})\)</span> over the vocabulary <span class="math notranslate nohighlight">\(V\)</span>, conditioned on the previous tokens <span class="math notranslate nohighlight">\(y_{&lt;t}\)</span>. The cross-entropy loss used during training can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{CE}} = -\sum_{t} \log P(y_t^* \mid y_{&lt;t})
\]</div>
<p>where <span class="math notranslate nohighlight">\(y_t^*\)</span> is the true token at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p><strong>Note:</strong> In this case, we can primarily deal with <strong>one-hot encoded sequences</strong>, selecting one token per time step, as we don’t have a simple way to sample <strong>N-hot vectors</strong> (where <span class="math notranslate nohighlight">\(N &gt; 1\)</span> tokens are selected simultaneously) from the model’s output distribution. Sampling multiple tokens at once would require modeling the joint probability of combinations of tokens, which significantly increases complexity and is not commonly addressed in standard sequence generation models.</p>
</section>
<section id="continuous-valued-data-generation">
<h4>Continuous-Valued Data Generation<a class="headerlink" href="#continuous-valued-data-generation" title="Permalink to this heading">#</a></h4>
<p>For generating <strong>continuous-valued data</strong>, the stochasticity usually comes from some form of noise injection into the neural network.
In the following, a brief (model-agnostic) introduction in the most common training paradigms is given.</p>
<section id="generative-adversarial-networks-gans">
<h5>Generative Adversarial Networks (GANs)<a class="headerlink" href="#generative-adversarial-networks-gans" title="Permalink to this heading">#</a></h5>
<p>For example, Generative Adversarial Networks (GANs) in their basic form inject noise by inputting a high-dimensional noise vector
<span class="math notranslate nohighlight">\(\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> (sampled from an independent Gaussian distribution) into the generator <span class="math notranslate nohighlight">\(G\)</span>.
The generator transforms this noise vector into a data sample: <span class="math notranslate nohighlight">\(\mathbf{x} = G(\mathbf{z})\)</span>.
Thus, the task can be described as learning to transform an independent Gaussian distribution into the data distribution.</p>
<p>The generator is trained by playing an adversarial game with a discriminator <span class="math notranslate nohighlight">\(D\)</span>.
The discriminator aims to distinguish between real samples from the dataset and fake samples generated by <span class="math notranslate nohighlight">\(G\)</span>.
The generator is trained to produce samples that maximize the likelihood of fooling the discriminator.
Formally, the objective for <span class="math notranslate nohighlight">\(G\)</span> is to minimize the following adversarial loss:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{G} = - \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} [\log(D(G(\mathbf{z})))]
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\mathbf{z})\)</span> represents the distribution of noise input.
This adversarial setup ensures that as <span class="math notranslate nohighlight">\(D\)</span> improves in distinguishing real from fake data,
<span class="math notranslate nohighlight">\(G\)</span> improves in generating more realistic samples, ultimately leading to convergence when the generated data becomes indistinguishable from the real data.</p>
</section>
<section id="variational-autoencoders-vaes">
<h5>Variational Autoencoders (VAEs)<a class="headerlink" href="#variational-autoencoders-vaes" title="Permalink to this heading">#</a></h5>
<p>Similarly, in Variational Autoencoders (VAEs, composed of encoder and decoder), the decoder receives as input a sample from an independent Gaussian prior distribution (a “standard normal distribution”).
The model is trained so that the encoder learns to approximate the prior using a mixture of Gaussian posteriors, one for each data point:
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}, \boldsymbol{\sigma} = E(\mathbf{x})\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}\)</span> are multi-dimensional mean and variance vectors.
From this posterior, we sample a latent variable <span class="math notranslate nohighlight">\(\mathbf{z} \sim q_{\phi}(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma})\)</span>.
The decoder then reconstructs the input by transforming <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> into a data point <span class="math notranslate nohighlight">\(\hat{\mathbf{x}} = D(\mathbf{z})\)</span>.</p>
<p>Training involves minimizing two objectives: the reconstruction loss between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\hat{\mathbf{x}}\)</span>, and the Kullback-Leibler (KL) divergence
between the learned posterior <span class="math notranslate nohighlight">\(q_{\phi}(\mathbf{z} \mid \mathbf{x})\)</span> and the prior distribution <span class="math notranslate nohighlight">\(p(\mathbf{z}) \sim \mathcal{N}(0, I)\)</span>.
The two objectives are adversarial because the KL term pushes the posteriors towards a zero mean and unit variance, while the reconstruction term encourages the posteriors to adopt distinct means and reduced variances, allowing each data point to have its own distribution.
Together, they make it possible to sample from the prior <span class="math notranslate nohighlight">\(p(\mathbf{z}) \sim \mathcal{N}(0, I)\)</span> at inference and decoding it into a plausible data sample: <span class="math notranslate nohighlight">\(\hat{\mathbf{x}} = D(\mathbf{z})\)</span>.</p>
</section>
<section id="diffusion-models">
<h5>Diffusion Models<a class="headerlink" href="#diffusion-models" title="Permalink to this heading">#</a></h5>
<p>In Diffusion Models, the noise input has the same dimensionality as the data point that should be generated.
The model gradually transforms noise into data through a series of steps.
Like before, the goal is to transform a Gaussian prior distribution into the data distribution through the learned denoising steps.</p>
<ol class="arabic">
<li><p><strong>Forward Diffusion (Noising) Process:</strong> Noise is added to the data over <span class="math notranslate nohighlight">\(T\)</span> time steps:</p>
<div class="math notranslate nohighlight">
\[
   q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
   \]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_t\)</span> is a variance schedule controlling the amount of noise added at each step.</p>
</li>
<li><p><strong>Reverse Diffusion (Denoising) Process:</strong> The model learns to reverse the noising process by estimating <span class="math notranslate nohighlight">\(p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_t)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
   p_{\theta}(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_{\theta}(\mathbf{x}_t, t), \sigma_t^2 \mathbf{I})
   \]</div>
</li>
</ol>
</section>
</section>
</section>
<section id="early-works">
<h3>Early Works<a class="headerlink" href="#early-works" title="Permalink to this heading">#</a></h3>
<p>Before the rise of Transformers and diffusion models, models like Causal Convolutional Networks, Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs) were used for musical audio generation.
At the time, it was common to generate in a low-level representation space, either directly in the signal domain (WaveNet, SampleRNN) or in the spectral domain (GANs).
Not least, due to their generation in such a high-dimensional space, CNNs/RNNs struggled with long-term dependencies, leading to repetitive or incoherent results without higher-level structure.<br />
GANs were used to generate audio in the signal or frequency domain but faced challenges with training instability and producing high-quality, diverse outputs.
Through the usage of neural audio codecs and the resulting reduction in dimensionality, the problem became simpler.
Nowadays, through a combination of more efficient/simpler to-train generative models with generation in a compressed space, it is possible to generate high-quality, full-length music tracks.</p>
<section id="wavenet">
<h4>WaveNet<a class="headerlink" href="#wavenet" title="Permalink to this heading">#</a></h4>
<p><img alt="wavenet_fig" src="_images/wavenet.png" /></p>
<p><strong>Figure 1:</strong> WaveNet architecture showing causal, dilated convolutions (image source: <span id="id1">[<a class="reference internal" href="bibiography.html#id14" title="Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In Alan W. Black, editor, The 9th ISCA Speech Synthesis Workshop, SSW 2016, Sunnyvale, CA, USA, September 13-15, 2016, 125. ISCA, 2016. URL: https://www.isca-archive.org/ssw\_2016/vandenoord16\_ssw.html.">vdODZ+16</a>]</span>).</p>
<p>WaveNet <span id="id2">[<a class="reference internal" href="bibiography.html#id14" title="Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. In Alan W. Black, editor, The 9th ISCA Speech Synthesis Workshop, SSW 2016, Sunnyvale, CA, USA, September 13-15, 2016, 125. ISCA, 2016. URL: https://www.isca-archive.org/ssw\_2016/vandenoord16\_ssw.html.">vdODZ+16</a>]</span> can be seen as the first successful attempt to directly generate audio using a Neural Network.
Important components in WaveNet are dilated convolutions <span id="id3">[<a class="reference internal" href="bibiography.html#id13" title="Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. 2016. URL: http://arxiv.org/abs/1511.07122.">YK16</a>]</span> that enable an exponentially growing receptive field with linearly increasing numbers of layers.
A big receptive field is critical in WaveNet because it operates directly in the signal domain with 16k samples/second.
In addition, causal convolutions are used to prevent the model from looking into the future during training, resulting in a generative autoregressive sequence model.</p>
<p>Autoregressive sequence models are typically trained with cross-entropy loss that requires one-hot encoded sequences.
As raw audio is usually 16-bit, a naive transformation into one-hot vectors would result in 65,536 dimensions per time step.
To keep the problem tractable, in WaveNet, each sample is non-linearly scaled and quantized to obtain 256-dimensional vectors.
The non-linear scaling function (ITU-T, 1988) is defined as</p>
<div class="math notranslate nohighlight">
\[
f(x_t) = \text{sign}(x_t) \frac{\ln(1 + \mu |x_t|)}{\ln(1 + \mu)}.
\]</div>
<p><img alt="wavenet_scaling" src="_images/wavenet_non-linearity.png" /></p>
<p><strong>Figure 2:</strong> Non-linear scaling of audio samples in WaveNet for <span class="math notranslate nohighlight">\(\mu = 255\)</span> (in practice, <span class="math notranslate nohighlight">\(-1 &lt; x_t &lt; 1\)</span>).</p>
<p><em>Usage Example</em>: WaveNet was used by Google for text-to-speech (TTS) applications.</p>
</section>
<section id="samplernn">
<h4>SampleRNN<a class="headerlink" href="#samplernn" title="Permalink to this heading">#</a></h4>
<p><img alt="sample_rnn" src="_images/sample_rnn.png" />
<strong>Figure 3:</strong> Snapshot of the unrolled SampleRNN model at timestep <span class="math notranslate nohighlight">\(i\)</span> with 3 tiers. As a simplification, only one RNN and up-sampling ratio <span class="math notranslate nohighlight">\(r = 4\)</span> is used for all tiers (image source: <span id="id4">[<a class="reference internal" href="bibiography.html#id12" title="Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C. Courville, and Yoshua Bengio. Samplernn: an unconditional end-to-end neural audio generation model. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL: https://openreview.net/forum?id=SkxKPDv5xl.">MKG+17</a>]</span>).</p>
<p>SampleRNN <span id="id5">[<a class="reference internal" href="bibiography.html#id12" title="Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C. Courville, and Yoshua Bengio. Samplernn: an unconditional end-to-end neural audio generation model. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL: https://openreview.net/forum?id=SkxKPDv5xl.">MKG+17</a>]</span> was the first RNN-based neural audio synthesizer that had an impact in the community.
It can effectively learn to generate long-form audio at a sample rate of 16kHz.
While <strong>WaveNet</strong> builds hierarchical representations of audio by its built-in sub-sampling through dilated convolutional layers,
SampleRNN builds such a hierarchy through multiple tiers of RNNs that operate in different “clock rates”.
This approach enables representations at varying temporal resolutions, where lower tiers (faster rates) are conditioned on higher tiers.
This encourages higher tiers to generate higher-level signal representations that help predict lower-level details.</p>
<p>Similarly to WaveNet, in order to keep the task tractable, the sample values are quantized to 256 bins—but without prior, non-linear scaling.
As the memory of RNNs can be updated iteratively without the need to reconsider past inputs, they tend to need less compute at inference time than non-recurrent autoregressive models (like causal convolutions or transformers).</p>
<p><em>Usage Example</em>: Different artists used SampleRNN for music generation. Notably, a <a class="reference external" href="https://www.youtube.com/watch?v=JF2p0Hlg_5U&amp;ab_channel=DADABOTS">livestream</a> (by Dadabots) with Technical Death Metal music is ongoing with hardly any interruptions since March 2019 <span id="id6">[<a class="reference internal" href="bibiography.html#id2" title="CJ Carr and Zack Zukowski. Generating albums with samplernn to imitate metal, rock, and punk bands. CoRR, 2018.">CZ18</a>]</span>.</p>
</section>
<section id="generative-adversarial-networks">
<h4>Generative Adversarial Networks<a class="headerlink" href="#generative-adversarial-networks" title="Permalink to this heading">#</a></h4>
<p>For several years, Generative Adversarial Networks (GANs) <span id="id7">[<a class="reference internal" href="bibiography.html#id11" title="Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, 2014. URL: http://arxiv.org/abs/1406.2661, arXiv:1406.2661.">GPougetAbadieM+14</a>]</span> were among the most influential generative models.
Their ability to implicitly model multi-dimensional <em>continuous-valued</em> distributions made them a compelling tool for image and audio generation.
This enabled the use of spectrogram (or spectrogram-like) representations in audio generation, which is a natural modality for 2D convolutional networks.
Another motivation for using image-like spectrogram representations with GANs for audio generation was the ability to leverage insights from the broader image-processing community.</p>
<p><img alt="gan_synth" src="_images/gansynth.png" />
<strong>Figure 4:</strong> GANSynth rainbowgrams to showcase the influence of different audio representations (image source: <span id="id8">[<a class="reference internal" href="bibiography.html#id10" title="Jesse H. Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam Roberts. Gansynth: adversarial neural audio synthesis. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL: https://openreview.net/forum?id=H1xQVn09FX.">EAC+19</a>]</span>).</p>
<p>While WaveGAN <span id="id9">[<a class="reference internal" href="bibiography.html#id9" title="Chris Donahue, Julian J. McAuley, and Miller S. Puckette. Adversarial audio synthesis. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL: https://openreview.net/forum?id=ByMVTsR5KQ.">DMP19</a>]</span> was an influential work on using GANs directly for raw musical audio waveform generation, most works focussed on spectrogram-like representations.
Examples for that are GANSynth <span id="id10">[<a class="reference internal" href="bibiography.html#id10" title="Jesse H. Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam Roberts. Gansynth: adversarial neural audio synthesis. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL: https://openreview.net/forum?id=H1xQVn09FX.">EAC+19</a>]</span>, SpecGAN <span id="id11">[<a class="reference internal" href="bibiography.html#id9" title="Chris Donahue, Julian J. McAuley, and Miller S. Puckette. Adversarial audio synthesis. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL: https://openreview.net/forum?id=ByMVTsR5KQ.">DMP19</a>]</span>, DrumGAN <span id="id12">[<a class="reference internal" href="bibiography.html#id7" title="Javier Nistal, Cyran Aouameur, Ithan Velarde, and Stefan Lattner. Drumgan VST: A plugin for drum sound analysis/synthesis with autoencoding generative adversarial networks. Proc. of International Conference on Machine Learning ICML, Workshop on Machine Learning for Audio Synthesis, MLAS, 2022, 2022.">NAVL22</a>, <a class="reference internal" href="bibiography.html#id8" title="Javier Nistal, Stefan Lattner, and Gaël Richard. DRUMGAN: synthesis of drum sounds with timbral feature conditioning using generative adversarial networks. In ISMIR, 590–597. 2020.">NLR20</a>]</span>, and DarkGAN <span id="id13">[<a class="reference internal" href="bibiography.html#id6" title="Javier Nistal, Stefan Lattner, and Gaël Richard. Darkgan: exploiting knowledge distillation for comprehensible audio synthesis with gans. In ISMIR, 484–492. 2021.">NLR21</a>]</span>, omitting those only applied to speech.
For simplicity reasons, the listed works can generate fixed-size outputs only. Some (later) examples of variable-size musical audio generation using GANs are VQCPC-GAN <span id="id14">[<a class="reference internal" href="bibiography.html#id5" title="Javier Nistal, Cyran Aouameur, Stefan Lattner, and Gaël Richard. VQCPC-GAN: variable-length adversarial audio synthesis using vector-quantized contrastive predictive coding. In WASPAA, 116–120. IEEE, 2021.">NALR21</a>]</span> and Musica! <span id="id15">[<a class="reference internal" href="bibiography.html#id4" title="Marco Pasini and Jan Schlüter. Musika! fast infinite waveform music generation. In ISMIR, 543–550. 2022.">PSchluter22</a>]</span>.</p>
<p>Presently, GANs are widely replaced by Diffusion Models, which are more stable in training, less prone to mode-collapse, and have a simpler architecture, resulting in higher-quality outputs.<br />
A concept of GANs that could remain in the mid-term is the usage of adversarial losses from auxiliary networks, for example, for domain confusion or as additional loss in reconstruction-based training (e.g., in neural audio codecs, like DAC <span id="id16">[<a class="reference internal" href="bibiography.html#id3" title="Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved RVQGAN. In NeurIPS. 2023.">KSL+23</a>]</span>).</p>
<p><em>Usage Example</em>: DrumGAN was the first commercially available neural audio synthesizer for music integrated into <a class="reference external" href="https://www.steinberg.net/vst-instruments/backbone/">Steinberg’s Backbone</a>. It is now available for free as an <a class="reference external" href="https://drumgan.csl.sony.fr/">online app</a>.</p>
</section>
</section>
<section id="autoregressive-transformer-architectures">
<h3>Autoregressive (Transformer) Architectures<a class="headerlink" href="#autoregressive-transformer-architectures" title="Permalink to this heading">#</a></h3>
<p>Autoregressive models, especially those based on Transformers, are well-suited for generating sequences like musical audio. These models generate audio by predicting each subsequent token based on prior ones, effectively capturing long-term relationships, which helps produce coherent compositions.</p>
</section>
<section id="id17">
<h3>Diffusion Models<a class="headerlink" href="#id17" title="Permalink to this heading">#</a></h3>
<p>Diffusion models offer another approach to musical audio generation. They transform random noise into meaningful continuous audio representations.</p>
</section>
</section>
<section id="how-is-the-task-evaluated">
<h2>How is the Task Evaluated?<a class="headerlink" href="#how-is-the-task-evaluated" title="Permalink to this heading">#</a></h2>
<p>Evaluation of generation tasks is difficult. In other ML tasks, specific targets (e.g., labels, data points) are available in a given evaluation set, allowing precision estimation for a given model. In contrast, in audio generation, the goal is to sample from the distribution of the training set without directly reproducing any training data.</p>
<p>As a result, indirect, distribution-based evaluation metrics are commonly used rather than relying on one-to-one comparisons, as in autoencoders or classification tasks.</p>
<section id="frechet-audio-distance-fad">
<h3>Frechet Audio Distance (FAD)<a class="headerlink" href="#frechet-audio-distance-fad" title="Permalink to this heading">#</a></h3>
<p>Nowadays, the most commonly used metric in audio generation is the Frechet Audio Distance (FAD). FAD compares the statistics of generated audio to those of real audio using embeddings from a pre-trained model. This metric measures how close the generated audio is to the original data distribution, which helps assess the quality and diversity of generated samples.</p>
</section>
<section id="subjective-evaluation">
<h3>Subjective Evaluation<a class="headerlink" href="#subjective-evaluation" title="Permalink to this heading">#</a></h3>
<p>Objective evaluation metrics cannot capture all the details people care about when listening to audio. Therefore, it is very common (and important) in audio generation works to perform user studies.
In user studies, participants might be asked to rate the quality of generated audio samples on a Likert Scale ranging from 1 (very poor) to 5 (excellent). This helps quantify subjective perceptions of audio quality, coherence, and musicality.</p>
</section>
<section id="likert-scale">
<h3>Likert Scale<a class="headerlink" href="#likert-scale" title="Permalink to this heading">#</a></h3>
<p>…</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="task_autotagging_ssl.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Auto-Tagging-SSL</p>
      </div>
    </a>
    <a class="right-next"
       href="bricks_input.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Inputs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-the-task">Goal of the Task</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-datasets">Popular Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-musical-audio-synthesis">Neural Musical Audio Synthesis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-generative-modeling">Basics of Generative Modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-sequence-generation">Discrete Sequence Generation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-valued-data-generation">Continuous-Valued Data Generation</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-autoencoders-vaes">Variational Autoencoders (VAEs)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-models">Diffusion Models</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-works">Early Works</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#wavenet">WaveNet</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#samplernn">SampleRNN</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-adversarial-networks">Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-transformer-architectures">Autoregressive (Transformer) Architectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Diffusion Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-the-task-evaluated">How is the Task Evaluated?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frechet-audio-distance-fad">Frechet Audio Distance (FAD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subjective-evaluation">Subjective Evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likert-scale">Likert Scale</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Geoffroy Peeters, Gabriel Meseguer-Brocal, Alain Riou, Stefan Lattner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>