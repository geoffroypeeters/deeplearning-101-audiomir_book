

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Generation with Latent Diffusion &#8212; Deep Learning 101 for Audio-based MIR</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'task_musicgeneration_diff';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inputs" href="bricks_input.html" />
    <link rel="prev" title="Autoregressive Generation" href="task_musicgeneration_auto.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="front.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/wave.png" class="logo__image only-light" alt="Deep Learning 101 for Audio-based MIR - Home"/>
    <script>document.write(`<img src="_static/wave.png" class="logo__image only-dark" alt="Deep Learning 101 for Audio-based MIR - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="front.html">
                    Deep Learning 101 for Audio-based MIR
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Abstract</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="abstract.html">Abstract</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="intro.html">Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="intro_dataset.html">Datasets .hdf5/.pyjama</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro_pytorch.html">Pytorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro_lightining.html">TorchLightning training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notebook.html">Notebooks in Colab</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tasks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musiccontent.html">Music Audio Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_multipitchestimation.html">Multi-Pitch-Estimation (MPE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_coverdetection.html">Cover Song Identification (CSI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_autotagging_frontend.html">Auto-Tagging (front-ends)</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_autotagging_ssl.html">Auto-Tagging (SSL)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="task_musicprocessing.html">Music Audio Processing</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="task_sourceseparation.html">Source Separation</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="task_musicgeneration.html">Musical Audio Generation</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_basics.html">Basics of Generative Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_early.html">Early Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="task_musicgeneration_auto.html">Autoregressive Generation</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Generation with Latent Diffusion</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Bricks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bricks_input.html">Inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_frontend.html">Front-ends</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_projection.html">Projections</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_bottleneck.html">Bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_architecture.html">Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="bricks_paradigm.html">Paradigms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="biography.html">About the authors</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibiography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Ftask_musicgeneration_diff.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/task_musicgeneration_diff.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Generation with Latent Diffusion</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#music2latent-codec">Music2Latent Codec</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-dataset">Preparing the dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-model-architecture"><strong>Diffusion Model Architecture</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-inference"><strong>Training and Inference</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#noise-addition-and-training-objective"><strong>Noise Addition and Training Objective</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-and-sampling-process-inference-function"><strong>3. Inference and Sampling Process (<code class="docutils literal notranslate"><span class="pre">inference</span></code> function)</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion"><strong>Conclusion</strong></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="generation-with-latent-diffusion">
<h1>Generation with Latent Diffusion<a class="headerlink" href="#generation-with-latent-diffusion" title="Permalink to this heading">#</a></h1>
<p>For non-autoregressive generation, Latent Diffusion is currently one of the most effective and popular approaches.
It focuses on creating high-quality samples by modeling data in a compressed, latent space rather than the spectrogram or waveform level.
This approach reduces computational cost and time by generating compact latent representations that, compared to raw data, are already simplified and organized.</p>
<p>Here, we provide a simple example of musical audio generation using the guitar subset of the NSynth dataset, Music2Latent representations, and a classic U-Net architecture.
In the corresponding <a class="reference external" href="https://github.com/geoffroypeeters/deeplearning-101-audiomir_notebook/blob/master/TUTO_task_Generation_Diffusion.ipynb">notebook</a> we implement a diffusion model that adopts a <strong>Rectified Flow</strong> method with <strong>ODE-based sampling</strong>.
This approach combines elements from both <strong>denoising diffusion probabilistic models (DDPMs)</strong> and <strong>normalizing flows</strong>, resulting in a continuous-time framework for generative modeling.</p>
<section id="music2latent-codec">
<h2>Music2Latent Codec<a class="headerlink" href="#music2latent-codec" title="Permalink to this heading">#</a></h2>
<p>Music2Latent (M2L) <span id="id1">[<a class="reference internal" href="bibiography.html#id39" title="Marco Pasini, Stefan Lattner, and George Fazekas. Music2latent: consistency autoencoders for latent audio compression. CoRR, 2024. URL: https://doi.org/10.48550/arXiv.2408.06500, arXiv:2408.06500, doi:10.48550/ARXIV.2408.06500.">PLF24</a>]</span> provides highly compressed, continuous audio representations with ~<span class="math notranslate nohighlight">\(11 \times 64\)</span>-dimensional vectors per second (for 44.1kHz sample rate).
A consistency autoencoder facilitates a <em>generative decoder</em> that makes up for potentially lost information, enabling high-quality reconstructions.
The M2L representations serve as the data space in which the diffusion process operates.
The diffusion model learns to generate M2L latent vectors, which are then decoded back into audio signals.</p>
<p><img alt="music2latent" src="_images/expe_generation_m2l.png" /></p>
<p><strong>Figure 1</strong>: Training process of Music2Latent. The input sample is first encoded into a sequence of latent vectors.
The latents are then upsampled with a decoder model.
The consistency model is trained via consistency training, conditioned on the information coming from the cross connections (image source: <span id="id2">[<a class="reference internal" href="bibiography.html#id39" title="Marco Pasini, Stefan Lattner, and George Fazekas. Music2latent: consistency autoencoders for latent audio compression. CoRR, 2024. URL: https://doi.org/10.48550/arXiv.2408.06500, arXiv:2408.06500, doi:10.48550/ARXIV.2408.06500.">PLF24</a>]</span>).</p>
<p>In our example, we first transform the dataset into M2L representations on which we then train our diffusion model.
This involves the following steps:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!pip install music2latent

import soundfile as sf
from music2latent import EncoderDecoder

# Initialize the encoder/decoder
encdec = EncoderDecoder()

# Load the audio file using soundfile
waveform, _ = sf.read(&quot;sample.wav&quot;)

# Encode using music2latent
latent = EncoderDecoder().encode(waveform)
</pre></div>
</div>
</section>
<section id="preparing-the-dataset">
<h2>Preparing the dataset<a class="headerlink" href="#preparing-the-dataset" title="Permalink to this heading">#</a></h2>
<p>In the following code, we download the dataset, intitialize the <code class="docutils literal notranslate"><span class="pre">MusicLatentDataset</span></code> and the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span># Download the dataset using git-lfs
!git clone https://github.com/SonyCSLParis/test-lfs.git
!bash ./test-lfs/download.sh NSYNTH_GUITAR_MP3

# Initialize the dataset and dataloader
audio_folder_train = &quot;./NSYNTH_GUITAR_MP3/nsynth-guitar-train&quot;
audio_folder_val = &quot;./NSYNTH_GUITAR_MP3/nsynth-guitar-valid&quot;

# When initializing the dataset, the data gets compressed using the M2L encoder
dataset = MusicLatentDataset(root_dir=audio_folder_train, encoder=encdec)
dataset_val = MusicLatentDataset(root_dir=audio_folder_val, encoder=encdec)

dataloader = DataLoader(dataset, batch_size=500, shuffle=True)
dataloader_val = DataLoader(dataset_val, batch_size=500, shuffle=False)
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="diffusion-model-architecture">
<h2><strong>Diffusion Model Architecture</strong><a class="headerlink" href="#diffusion-model-architecture" title="Permalink to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">DiffusionUnet</span></code> class defines a <a class="reference internal" href="bricks_architecture.html#lab-unet"><span class="std std-ref">classic U-Net architecture</span></a> with time conditioning.
The time embedding (<code class="docutils literal notranslate"><span class="pre">self.time_mlp</span></code>) is a multi-layer perceptron (MLP) that embeds the time variable <span class="math notranslate nohighlight">\(t\)</span> into a higher-dimensional space to condition the model on the diffusion time step.
It is added to the deepest layer, allowing the model to adjust its predictions based on the amount of noise present.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="c1"># ...</span>
    
    <span class="c1"># Bottleneck</span>
    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

    <span class="c1"># Time embedding</span>
    <span class="n">t_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_mlp</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [batch_size, channels]</span>
    <span class="n">t_emb</span> <span class="o">=</span> <span class="n">t_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>             <span class="c1"># [batch_size, channels, 1]</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">t_emb</span>                           <span class="c1"># Broadcast addition</span>
    <span class="c1"># ... </span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="training-and-inference">
<h2><strong>Training and Inference</strong><a class="headerlink" href="#training-and-inference" title="Permalink to this heading">#</a></h2>
<p>In this section, we show the</p>
<ol class="arabic">
<li><p><strong>Noise Addition via Linear Interpolation</strong>: The model adds noise to the data by linearly interpolating between the clean data <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> and pure noise <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, controlled by a time variable <span class="math notranslate nohighlight">\(t\)</span> as</p>
<div class="math notranslate nohighlight">
\[
   \mathbf{x}_t = (1 - t) \mathbf{x}_0 + t \cdot \mathbf{z}, \quad t \in [0, 1],
   \]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> is the time step indicating the level of noise added, with <span class="math notranslate nohighlight">\(t = 0\)</span> being no noise and <span class="math notranslate nohighlight">\(t = 1\)</span> being full noise.
This formulation is commonly used in Rectified Flow models and allows a simple, linear interpolation between the data and pure noise.</p>
</li>
<li><p><strong>Training Objective</strong>: The model is trained to predict the residual <span class="math notranslate nohighlight">\(\mathbf{v} = \mathbf{x}_t - \mathbf{z}\)</span> from the noisy samples and the time <span class="math notranslate nohighlight">\(t\)</span>. This residual guides the denoising process.
In the case of Rectified Flow with ODE-based sampling, the reverse process can be formulated as:</p>
<div class="math notranslate nohighlight">
\[
   \frac{d\mathbf{x}}{dt} = f(\mathbf{x}_t, t),
   \]</div>
<p>where <span class="math notranslate nohighlight">\(f(\mathbf{x}_t, t)\)</span> is the learned denoising function that predicts the residual (difference between noisy data and clean data):</p>
<div class="math notranslate nohighlight">
\[
   f(\mathbf{x}_t, t) \approx \mathbf{x}_t - \mathbf{z}.
   \]</div>
<p>As the model is trained to learn the difference between the data and the <em>unscaled</em> noise, it effectively learns a vector field with vectors pointing towards positions of high density (high probability).</p>
</li>
<li><p><strong>ODE-Based Sampling</strong>: During inference, the model uses an Ordinary Differential Equation (ODE) solver (the trained U-Net) to integrate over time from <span class="math notranslate nohighlight">\(t = 1\)</span> (pure noise) to <span class="math notranslate nohighlight">\(t = 0\)</span> (clean data).
As the model outputs unscaled direction vectors, at inference they are scaled by <span class="math notranslate nohighlight">\(\Delta t = \frac{1}{\text{num_steps}}\)</span> (the <code class="docutils literal notranslate"><span class="pre">step_size</span></code>).
Note that, given the initial noise <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, this process is deterministic.
This is in contrast to Denoising Diffusion Probabilistic Models (DDPMs) based on Stochastic Differential Equations (SDEs), where the noise is sampled at every step of the reverse diffusion process.</p></li>
</ol>
<section id="noise-addition-and-training-objective">
<h3><strong>Noise Addition and Training Objective</strong><a class="headerlink" href="#noise-addition-and-training-objective" title="Permalink to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">RectifiedFlows</span></code> class handles the noise addition and defines the training loss:</p>
<ul>
<li><p><strong>Noise Addition (<code class="docutils literal notranslate"><span class="pre">add_noise</span></code> method)</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add_noise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">times</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">times</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">times</span> <span class="o">*</span> <span class="n">noise</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">add_noise</span></code> performs a linear interpolation between clean data <code class="docutils literal notranslate"><span class="pre">x</span></code> and random noise <code class="docutils literal notranslate"><span class="pre">noise</span></code> based on the time variable <code class="docutils literal notranslate"><span class="pre">times</span></code>.</p>
</li>
<li><p><strong>Time Variable Sampling</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">times</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">P_std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">P_mean</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The sigmoid non-linearity ensures <code class="docutils literal notranslate"><span class="pre">times</span></code> lies between 0 and 1.</p>
</li>
<li><p><strong>Training Objective (<code class="docutils literal notranslate"><span class="pre">forward</span></code> method)</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_loss</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="n">noises</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">noises</span>
    <span class="n">noisy_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_noise</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noises</span><span class="p">,</span> <span class="n">times</span><span class="p">)</span>
    <span class="n">fv</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">noisy_samples</span><span class="p">,</span> <span class="n">times</span><span class="p">,</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">fv</span><span class="p">)</span>
    <span class="c1"># ...</span>
</pre></div>
</div>
</li>
</ul>
<p>The model calculates the residual by subtracting the noise from the data, expressed as <span class="math notranslate nohighlight">\(v = x - \text{noise}\)</span>.
It then predicts <span class="math notranslate nohighlight">\(fv\)</span>, an approximation of the residual <span class="math notranslate nohighlight">\(v\)</span>, based on the noisy samples and the corresponding time step.
The loss function used is the Mean Squared Error (MSE) between the true residual <span class="math notranslate nohighlight">\(v\)</span> and the predicted residual <span class="math notranslate nohighlight">\(fv\)</span>, which is given by <span class="math notranslate nohighlight">\(\text{Loss} = \| v - \text{fv} \|^2\)</span>.
Predicting the residual instead of the noise or the data directly is a characteristic of the <strong>Rectified Flow</strong> method, while in <strong>Denoising Diffusion Probabilistic Models (DDPMs)</strong>, the model typically predicts the noise.</p>
</section>
<section id="inference-and-sampling-process-inference-function">
<h3><strong>3. Inference and Sampling Process (<code class="docutils literal notranslate"><span class="pre">inference</span></code> function)</strong><a class="headerlink" href="#inference-and-sampling-process-inference-function" title="Permalink to this heading">#</a></h3>
<p>During inference, the model generates new samples by solving an ODE:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">rectified_flows</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">latents_shape</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">):</span>
    <span class="c1"># Initialize with pure noise</span>
    <span class="n">current_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">latents_shape</span><span class="p">)</span>
    <span class="n">times</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">latents_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">step_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">num_steps</span>
    <span class="c1"># Integrate over time</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">current_sample</span><span class="p">,</span> <span class="n">times</span><span class="p">)</span>
        <span class="n">current_sample</span> <span class="o">=</span> <span class="n">current_sample</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">v</span>
        <span class="n">times</span> <span class="o">=</span> <span class="n">times</span> <span class="o">-</span> <span class="n">step_size</span>
    <span class="k">return</span> <span class="n">current_sample</span> <span class="o">/</span> <span class="n">sigma_data</span>
</pre></div>
</div>
<ul>
<li><p><strong>Initialization</strong>:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">current_sample</span></code></strong>: Starts as pure noise.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">times</span></code></strong>: Begins at <span class="math notranslate nohighlight">\(t = 1\)</span>, representing the highest noise level.</p></li>
</ul>
</li>
<li><p><strong>Integration Loop</strong>:</p>
<ul>
<li><p><strong>Time Step (<code class="docutils literal notranslate"><span class="pre">step_size</span></code>)</strong>: Calculated as <span class="math notranslate nohighlight">\(\Delta t = \frac{1}{\text{num_steps}}\)</span>.</p></li>
<li><p><strong>Euler’s Method</strong>: Updates the sample by moving in the direction of <code class="docutils literal notranslate"><span class="pre">v</span></code> predicted by the model:</p>
<p><span class="math notranslate nohighlight">\(
\text{current_sample} = \text{current_sample} + \Delta t \cdot v
\)</span></p>
</li>
<li><p><strong>Time Update</strong>: <span class="math notranslate nohighlight">\(t = t - \Delta t\)</span></p></li>
</ul>
</li>
<li><p><strong>Result</strong>: After integrating from <span class="math notranslate nohighlight">\(t = 1\)</span> to <span class="math notranslate nohighlight">\(t = 0\)</span>, the <code class="docutils literal notranslate"><span class="pre">current_sample</span></code> approximates a sample from the data distribution.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="conclusion">
<h2><strong>Conclusion</strong><a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h2>
<p>In this tutorial, we demonstrated a practical implementation of musical audio generation using Latent Diffusion with the guitar subset of the NSynth dataset and Music2Latent (M2L) representations.
By encoding audio data into compressed latent vectors through M2L, we significantly reduced computational complexity.</p>
<p>We employed a diffusion model that adopts the <strong>Rectified Flow</strong> method with <strong>ODE-based deterministic sampling</strong>.
This approach merges concepts from both denoising diffusion probabilistic models (DDPMs) and normalizing flows into a continuous-time generative modeling framework.</p>
<p>Specifically, the model:</p>
<ul class="simple">
<li><p><strong>Trains to predict residuals</strong> between the data and noise, effectively learning a vector field pointing toward regions of high data density. This residual prediction guides the denoising process during generation.</p></li>
<li><p><strong>Utilizes ODE-based deterministic sampling</strong> during inference, integrating over time from pure noise (<span class="math notranslate nohighlight">\(t = 1\)</span>) to clean data (<span class="math notranslate nohighlight">\(t = 0\)</span>) without introducing additional randomness at each step. This results in a more efficient and stable generation process compared to stochastic methods.</p></li>
<li><p><strong>Incorporates a time-conditioned U-Net architecture</strong> that adapts its predictions based on the noise level at each time step. The time embedding allows the model to handle varying noise levels.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="task_musicgeneration_auto.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Autoregressive Generation</p>
      </div>
    </a>
    <a class="right-next"
       href="bricks_input.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Inputs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#music2latent-codec">Music2Latent Codec</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-dataset">Preparing the dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-model-architecture"><strong>Diffusion Model Architecture</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-inference"><strong>Training and Inference</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#noise-addition-and-training-objective"><strong>Noise Addition and Training Objective</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-and-sampling-process-inference-function"><strong>3. Inference and Sampling Process (<code class="docutils literal notranslate"><span class="pre">inference</span></code> function)</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion"><strong>Conclusion</strong></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Geoffroy Peeters, Gabriel Meseguer-Brocal, Alain Riou, Stefan Lattner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>